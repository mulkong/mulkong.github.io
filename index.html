<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.0/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"mulkong.github.io","root":"/","scheme":"Muse","version":"8.0.1","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}};
  </script>

  <meta name="description" content="공부한 내용을 바탕으로 정리한 기술블로그 입니다.">
<meta property="og:type" content="website">
<meta property="og:title" content="Mulkong DeepLearning">
<meta property="og:url" content="http://mulkong.github.io/index.html">
<meta property="og:site_name" content="Mulkong DeepLearning">
<meta property="og:description" content="공부한 내용을 바탕으로 정리한 기술블로그 입니다.">
<meta property="og:locale" content="ko_KR">
<meta property="article:author" content="mulkong">
<meta property="article:tag" content="DeepLearning, AI, Pytorch, Python, MachineLearning, Programming">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://mulkong.github.io/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'ko'
  };
</script>

  <title>Mulkong DeepLearning</title>
  






  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Mulkong DeepLearning</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">딥러닝 정리 블로그</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>홈</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>태그</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>카테고리</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>아카이브</a>

  </li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          목차
        </li>
        <li class="sidebar-nav-overview">
          흝어보기
        </li>
      </ul>

      <!--noindex-->
      <section class="post-toc-wrap sidebar-panel">
      </section>
      <!--/noindex-->

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">mulkong</p>
  <div class="site-description" itemprop="description">공부한 내용을 바탕으로 정리한 기술블로그 입니다.</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">8</span>
          <span class="site-state-item-name">포스트</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">카테고리</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">태그</span>
      </div>
  </nav>
</div>



      </section>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">
      

      
    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="ko">
    <link itemprop="mainEntityOfPage" href="http://mulkong.github.io/2022/03/13/Adversarial-Latent-Autoencoders-md/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="mulkong">
      <meta itemprop="description" content="공부한 내용을 바탕으로 정리한 기술블로그 입니다.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mulkong DeepLearning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/03/13/Adversarial-Latent-Autoencoders-md/" class="post-title-link" itemprop="url">Adversarial Latent Autoencoders</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">작성일</span>
      

      <time title="Post created: 2022-03-13 17:09:42 / Updated at: 17:31:26" itemprop="dateCreated datePublished" datetime="2022-03-13T17:09:42+09:00">2022-03-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/GAN/" itemprop="url" rel="index"><span itemprop="name">GAN</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <hr>
<ul>
<li>본 논문은, GAN의 Generator를 F, G로 Discriminator를 E, D로 각각 분리해서 연구를 진행했습니다.</li>
<li>F에서 나온 Latent space와 E를 통해 나온 latent space가 서로 동일한 분포를 가진다고 가정을 한 상태로 연구가 진행 되었으며, F는 deterministic하게 latent space mapping이 이루어지고, G와 E는 독립적이고 이미 알려진 분포의 노이즈인 \( \eta\)를 입력으로 주어 stochastic하게 만듭니다.</li>
<li>결론적으로 Latent space의 확률 분포를 Adversarial하게 학습할 수 있는 장점이 있으며 그로 인하여 GAN과 비슷한 생성 능력을 지니고 disentangled representations을 학습 한 점을 보여주고 있습니다.</li>
</ul>
<hr>
<ul>
<li>Adversarial Latent Autoencoders에 대해 간단하게 리뷰를 해본 후 다음 포스팅에서는 논문을 바탕으로 code에서 어떤식으로 구현이 되었는지 살펴보도록 하겠습니다.</li>
</ul>
<p>✔️ [논문 리뷰] Adversarial Latent Autoencoders<br>◻︎ [코드 리뷰] Adversarial Latent Autoencoders</p>
<hr>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><hr>
<p><strong>Autoencoder의 목표</strong><br>Autoencoder는 encoder-generator map을 동시에 학습하여 generation and representation properties을 결합하는 것을 목표로 하는 unsupervised 접근 방식 입니다.</p>
<p><strong>Autoencoder의 한계</strong><br>Autoencoder에 대한 많은 연구가 진행되었지만, GANs(<em>Generative Adversarial Networks</em>)과 동일한 생성 능력을 가지고 있는지, 아니면 disentangled representations을 학습하는지에 대한 문제는 완전히 다루어지지 않았습니다.</p>
<p><strong>제안한 방법</strong><br>본 논문에서는 위에서 언급한 문제인</p>
<ul>
<li><code>GANs와 동일한 생성 능력을 가지고 있는지</code></li>
<li><code>disentangled representations을 학습하고 있는지</code></li>
</ul>
<p>에 대한 문제를 해결하는 autoencoder를 소개하며 저자들은 이 네트워크를 <code>Adversarial Latent Autoencoder</code>라고 부릅니다.</p>
<p><strong>2가지의 autoencoder 설계</strong><br>본 논문에서는 2가지의 autoencoder를 설계했습니다.</p>
<ul>
<li><code>MLP Encoder를 기반으로 하는 encoder</code></li>
<li><code>StyleGAN의 Generator를 기반으로 하는 StyleALAE</code></li>
</ul>
<p>설계된 두가지의 아키텍처의 <code>disentanglement properties</code>를 확인합니다.</p>
<p>StyleALAE는 StyleGAN과 비슷한 품질의 1024x1024 얼굴 이미지를 생성할 수 있으며, 동일한 해상도에서 실제 이미지를 기반으로 face <code>reconstructions</code>, <code>manipulations</code>도 생성할 수 있음을 보여주고 있습니다. </p>
<blockquote>
<p>이로써 이 논문에서 해결하고자 하는 (1) GANs와 동일한 생성 능력을 가지고 있는지. (2) 네트워크가 disentangled representations을 학습하고 있는지에 대한 문제를 해결했다는 것을 알 수 있습니다.</p>
</blockquote>
<p>따라서 ALAE는 GAN의 성능과 비슷하거나 그 성능을 능가하는 최초의 Autoencoder라고 소개하고 있습니다.</p>
<p></br></br></p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h1><hr>
<p>지금까지 발표된 이미지 생성 네트워크를 보면 아래와 같이 크게 두가지 특징을 관심있게 볼 수 있다고 생각할 수 있습니다.</p>
<ul>
<li>GAN처럼 얼마나 고해상도 이미지를 실제와 똑같게 생성할 수 있는지</li>
<li>disentangled representations 학습이 잘 이루어졌는지.</li>
</ul>
<p>지금까지 진행된 연구들의 논문들을 보면 아직까지는 고해상도 이미지를 생성하는 네트워크는 Autoencoder 보다는 GAN이라고 다들 이렇게 생각하실겁니다. 또한 Autoencoder는 지금까지는 entangled representations을 배워 manipulations가 불가능했습니다.</p>
<p>이 논문의 저자들은, 이때당시 나온 GAN의 발전 동향, 특히 StyleGAN의 아이디어를 응용하여 위에서 언급한 근본적인 한계들을 해결하는 new AutoEncoder를 제안하고 있습니다.</p>
<p>StyleALAE의 생성 결과부터보면 아래 그림 1처럼 GAN처럼 고해상도 이미지를 잘 생성하고 있는것을 보여주고 있습니다.</p>
<p><img src="/images/post_images/post0007/figure0001.png"> <center><em>그림 1. FFHA generation. Generations with StyleALAE trained on FFHQ at 1024 x 1024</em></center></p>
<p></br></br></p>
<h1 id="2-Preliminaries"><a href="#2-Preliminaries" class="headerlink" title="2. Preliminaries"></a>2. Preliminaries</h1><hr>
<h2 id="AutoEncoders"><a href="#AutoEncoders" class="headerlink" title="AutoEncoders"></a>AutoEncoders</h2><p><img src="/images/post_images/post0007/figure0002.png"> <center><em>그림 2. Autoencoder architecture (출처:<a target="_blank" rel="noopener" href="https://learnopencv.com/variational-autoencoder-in-tensorflow/">https://learnopencv.com/variational-autoencoder-in-tensorflow/</a>)</em></center></p>
<h5 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h5><p>입력된 이미지에 대한 특징(feature)인 고차원 input space을 저차원 latent space \( z\)로 압축하는 모델이며 식은 다음과 같습니다.<br>$$ z = E(x) $$</p>
<h5 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h5><p>Encoder에 의해 고차원에서 저차원으로 인코딩된 \( z\)를 다시 입력된 이미지와 동일한 도메인으로 재구성 하는 모델이며 식은 다음과 같습니다.<br>$$ \hat{x} = D(z) $$</p>
<h2 id="GANs"><a href="#GANs" class="headerlink" title="GANs"></a>GANs</h2><hr>
<p><img src="/images/post_images/post0007/figure0003.png"> <center><em>그림 3. GANs architecture (출처: <a target="_blank" rel="noopener" href="https://developers.google.com/machine-learning/gan/gan_structure">https://developers.google.com/machine-learning/gan/gan_structure</a>)</em></center></p>
<h5 id="GAN"><a href="#GAN" class="headerlink" title="GAN"></a>GAN</h5><p>Generator와 Discriminator로 구성되어있으며 서로 Adversarial Training 하는 방식으로 고해상도 이미지를 생성하도록 학습하는 생성모델 입니다. 이는 \( q(x)\)가 \( P_D(x)\)만큼 가깝도록 G를 학습 하는것을 목표로 하게 되며 다른 의미로는 G가 D를 속여서 D가 진짜인지 가짜인지 헷갈리게 만들도록 학습 되는것을 목표로 합니다.</p>
<h5 id="Generator"><a href="#Generator" class="headerlink" title="Generator"></a>Generator</h5><p>Autoencoder처럼 입력 이미지를 latent space로 직접 인코딩 하는 방식이 아니라, <strong><em>synthetic distribution \( q(x)\)를 표현하는 새로운 이미지를 생성하기 위해 이전에 알려진 \( p(z)\)의 latent space \( Z\)을 학습하는 네트워크</em></strong> 입니다.</p>
<h5 id="Discriminator"><a href="#Discriminator" class="headerlink" title="Discriminator"></a>Discriminator</h5><p>학습 데이터셋의 True distribution \( P_D(x)\)를 나타내는 이미지와 생성된 이미지를 구분하는 네트워크 입니다. 즉, 진짜인지 가짜인지 구분하는 binary classifier라고도 볼 수 있습니다.</p>
<h2 id="Adversarial-Latent-Autoencoders"><a href="#Adversarial-Latent-Autoencoders" class="headerlink" title="Adversarial Latent Autoencoders"></a>Adversarial Latent Autoencoders</h2><hr>
<p>Autoencoder는 지금까지 많은 연구가 진행되었지만 크게 2가지 문제에 대해서는 아직 해결을 못한 상태라고 볼 수 있습니다. (<em>자주 언급되고 있는 내용이지만 다시 한번 언급을 하도록 하겠습니다.</em>)</p>
<ul>
<li><code>GANs와 비슷한 생성 능력을 가지고 있는가?</code></li>
<li><code>disentangled representations을 잘 학습하였는가?</code></li>
</ul>
<p>이 논문 저자는 위에서 언급한 두가지 문제를 모두 해결할 수 있는 AutoEncoder를 설계했으며 자세한 아키텍처는 아래에서 다루도록 하겠습니다.</p>
<p></br></br></p>
<h1 id="3-Architecture"><a href="#3-Architecture" class="headerlink" title="3. Architecture"></a>3. Architecture</h1><hr>
<h3 id="ALAE"><a href="#ALAE" class="headerlink" title="ALAE"></a>ALAE</h3><p><img src="/images/post_images/post0007/figure0004.png"> <center><em>그림 4. ALAE Architecture. Architecture of an Adversarial Latent Autoencoder.</em></center></p>
<p>ALAE는 generator <strong>G</strong>, discriminator <strong>D</strong>를 \(\textit{G} = F \circ G \) and \( \textit{D} = D \circ E \)로 decomposing한 네트워크 입니다. 이때 decomposed 네트워크 사이의 latent space 즉, \(F(z)\)를 통해 나온 \( w\)와 discriminator E인 \(q_G(x\vert w, \eta)\)에 의해서 나온 \( w\)가 서로 동일하다는 가정을 지니고 있느며 이 latent space를 \( \mathcal{W}\)로 표현하고 있습니다.</p>
<h4 id="1-Generator-F-code"><a href="#1-Generator-F-code" class="headerlink" title="1) Generator F [code]"></a>1) Generator <strong>F</strong> <a target="_blank" rel="noopener" href="https://github.com/podgorskiy/ALAE/blob/42bcf2e5f213ff1c919483678344f3da6bc90f8a/model.py#L44">[code]</a></h4><p><strong>F</strong>는 prior distribution \( p(z) \to \) intermediate latent space \( w \) distribution \( q_F(w)\)로 변환하는 역할을 가지고 있습니다.</p>
<p><img src="/images/post_images/post0007/figure0005.png"> <center><em>그림 4. intermediate latent space로 변환하는 역할인 F</em></center></p>
<p>이 논문에서 <strong>disentaglement</strong>에 대해 아주 중요한 이야기를 하고 있습니다.</p>
<blockquote>
<p>“<code>input space에서 멀리 떨어진 intermediate latent space 가 더 나은 disentanglement properties를 갖는 경향</code>이 있음을 보여주었다.”</p>
</blockquote>
<p>따라서 저자는, <strong>F</strong>가 most general cases에서 deterministic map이라고 가정했습니다. 따라서 <strong>F</strong>는 know prior p(z)에서 샘플을 가져와서 q_F(w)을 출력하게 됩니다.</p>
<p><em>deterministic mapping에 대해 궁금하신 분들은 해당 링크를 참고해주시기 바랍니다. <a target="_blank" rel="noopener" href="https://wikidocs.net/3413">[link]</a></em></p>
</br>

<h4 id="2-Generator-G-code"><a href="#2-Generator-G-code" class="headerlink" title="2) Generator (G) [code]"></a>2) Generator (G) <a target="_blank" rel="noopener" href="https://github.com/podgorskiy/ALAE/blob/42bcf2e5f213ff1c919483678344f3da6bc90f8a/model.py#L51">[code]</a></h4><p>기존 original GAN과 차이점부터 비교해가며 알아보면,</p>
<h5 id="📌-GAN"><a href="#📌-GAN" class="headerlink" title="📌 GAN"></a>📌 GAN</h5><ul>
<li>Generator에 대한 입력은 latent space에서 직접 샘플링 </li>
<li>생성된 이미지는 binary classifier처럼 진짜/가짜로 분류해주는 Discriminator에 바로 입력.<h5 id="📌-ALAE"><a href="#📌-ALAE" class="headerlink" title="📌 ALAE"></a>📌 ALAE</h5></li>
<li><strong>F</strong>에서 학습된 intermediate latent space \( w\)에서 샘플링</li>
<li>G에서 생성된 이미지가 Encoder를 먼저 거친 뒤 Discriminator에 입력.</li>
</ul>
<p>저자는 G가 known fixed distribution \( p_\eta(\eta)\)에서 샘플링된 independent noisy input \( \eta\)에 optionally 하게 의존할 수 있다고 가정하고 있습니다. 따라서 G에 대한 입력은 \( q_F(w)\)와 optionally \( p_\eta(\eta)\) 입니다. 그래서 해당 식은 다음과 같이 정의됩니다.</p>
<p><img src="/images/post_images/post0007/figure0006.png"> <center><em>그림 6. Generator G에 대한되는 영역.</em></center></p>
<p>💡 \(q_G(x\vert w, \eta)\): \(w\) and \(\eta\)가 주어지면 생성된 이미지 \(x \)의 조건부 확률(conditional probability)</p>
<p><img src="/images/post_images/post0007/figure0007.png"> <center><em>그림 7. Discriminator Encoder E에 해당되는 영역</em></center></p>
</br>

<h5 id="3-Discriminator-Encoder-E"><a href="#3-Discriminator-Encoder-E" class="headerlink" title="3) Discriminator Encoder(E)"></a>3) Discriminator Encoder(E)</h5><p>위에서 언급했던대로 decomposed 네트워크 사이의 latent space가 서로 동일하다는 가정을 지니고 있느며 latent space를 \( w\)로 표현하고 있음을 언급 했습니다. </p>
<p>즉, intermediate latent space \( w\)로 data space를 인코딩 하며 이는 \( \textit{G} = F \circ G\) and \( D = E \circ D\) 둘다 동일한 latent space을 갖습니다.</p>
<p>학습중에 Encoder에 대한 입력은 실제 데이터 분포 \( P_D(x)\)의 real image 또는 synthetic distribution q(x)를 나타내는 생성 이미지 입니다.</p>
<img src="/images/post_images/post0007/figure0008.png" width="100%" height="100%">

<p>synthetic distribution에서 입력을 가지고올 때 encoder의 출력을 다음 식과 같습니다.</p>
<img src="/images/post_images/post0007/figure0009.png" width="100%" height="100%">

<p>\( q_E(w)\): 주어진 data space에서 latent space \(w \)의 conditional probability distribution</p>
</br>

<p>실제 데이터 분포 \( P_D(x)\)에서 입력을 가져올 때 Encoder의 출력은 아래 식과 같습니다.</p>
<p>$$ q_{E, D}(w) = \int_{x} q(w \vert x) P_D(x) dx$$</p>
<blockquote>
<p>위와 같은 식이 성립되는 이유는, <strong>F</strong>에서 나온 intermediate latent space \( w\)와 <strong>E</strong>에서 나온 intermediate latent sapce \( w\)가 동일하다는 가정하에 연구가 진행된 것 입니다. 따라서 아래와 같은 식이 성립됩니다.</p>
</blockquote>
<p>$$ q_F(w) = q_E(w) $$</p>
<p>ALAE는 adversarial strategy로 학습이 진행됩니다. 따라서 \( q(x)\) → \( P_D(x)\)가 되며 이는 \(q_E(x)\) → \(q_{E, D}(x)\)로 이동하는 것을 암시합니다.</p>
</br>

<h6 id="3-1-Matching-the-latent-space"><a href="#3-1-Matching-the-latent-space" class="headerlink" title="3-1) Matching the latent space"></a>3-1) Matching the latent space</h6><p>latenet space에 대한 가정은 \(q_E(w)\)의 output distribution이 \(q_F(w)\)의 input distribution과 유사하다는 점 입니다. 이 개념을 바탕으로 실제 학습을 진행할 때 두 분포간의 squared difference를 최소화 하는 방향으로 학습을 진행하게 됩니다.</p>
</br>

<p>일반적인 autoencoder에서 입력된 이미지와 Decoder에 의해 복원된 이미지를 이용하여reconstruction loss을 구할 때 사용된 loss는 <code>L2 loss</code> 입니다. L2 loss는 data space에서 연산이 이루어지만, human visual perception을 반영하지 않는다는 특징이 있습니다. 이러한 이유로 인해 autoencoder가 GAN과 같은 선명한 이미지를 생성할 수 없게 되는 이유 중 하나 입니다.</p>
</br>

<h5 id="4-Discriminator-D"><a href="#4-Discriminator-D" class="headerlink" title="4. Discriminator D"></a>4. Discriminator D</h5><p>Encoder에 의해 제공되는 intermediate latent space가 입력되며 진짜인지 가짜인지 판별해주는 역할을 하는 네트워크 입니다.</p>
<p>D는 학습 과정에서 2번 호출됩니다.</p>
<ul>
<li>G에 의해 생성된 이미지 \(q_G(x\vert w, \eta)\)가 E를 통해 latent space mapping이 이루어지고, 그 latent space \( q_E(w)\)가 D에 제공되는 경우.</li>
<li>실제 데이터 \( x\)가 E에 입력되고 그로 인해 나오는 출력이 D에 제공되는 경우.</li>
</ul>
<blockquote>
<p>이건 그냥 일반적인 GAN 학습 방법에서 Discriminator 역할을 떠올리시면 됩니다.</p>
</blockquote>
<hr>
<h2 id="StyleALAE"><a href="#StyleALAE" class="headerlink" title="StyleALAE"></a>StyleALAE</h2></br>

<p>위 내용까지는 ALAE에 대한 특징 및 아키텍처에 대해 알아보았습니다. ALAE의 기본 아키텍처에다가 StyleGAN에 적용하여 StyleGAN과 거의 비슷한 성능을 보이는 StyleALAE 아키텍처에 대해 간단하게 알아보도록 하겠습니다.</p>
<p><img src="/images/post_images/post0007/figure0010.png"> <center><em>그림 8. StyleALAE Architecture</em></center></p>
<p>StyleALAE는 StyleGAN의 Generator 부분과 ALAE를 결합한 아키텍처 입니다. 따라서 StyleALAE는 다은과 같이 구성됩니다.</p>
<ul>
<li>(그림 9)와 같이 ALAE의 Generator가 StyleGAN의 Generator로 변경.</li>
<li>style information을 추출하는 Encoder network E는 Generator와 대칭이 되도록 구성 됩니다.</li>
</ul>
<p><img src="/images/post_images/post0007/figure0011.png"> <center><em>그림 9</em></center></p>
<p><strong>IN(Instance Normalization)</strong><br>각 레이어 \( i^{th} \)의 Style content를 추출하며, 입력된 영상에 대한 Normalization을 진행합니다.</p>
<p><strong>Style Information</strong><br>IN에 의해 추출된 style content를 구성하는 channel-wise average \( \mu \)와 \( \sigma\)를 의미 합니다.</p>
<p>Encoder는 \( i^{th}\)의 style content들은 latent space \( w\)와 선형적으로 관련된 symmetric generator(G of StyleALAE)의 AdaIN(Adaptive Instance Normalization) 으로 입력 됩니다. 따라서, Encoder의 각 레이어의 IN에 의해 추출된 style content는 multilinear map을 통해 latent space에 mapping 됩니다.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="ko">
    <link itemprop="mainEntityOfPage" href="http://mulkong.github.io/2022/01/01/Dual-Contrastive-Learning-for-Unsupervised-Image-to-Image-Translation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="mulkong">
      <meta itemprop="description" content="공부한 내용을 바탕으로 정리한 기술블로그 입니다.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mulkong DeepLearning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/01/01/Dual-Contrastive-Learning-for-Unsupervised-Image-to-Image-Translation/" class="post-title-link" itemprop="url">Dual Contrastive Learning for Unsupervised Image-to-Image Translation</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">작성일</span>

      <time title="Post created: 2022-01-01 18:59:13" itemprop="dateCreated datePublished" datetime="2022-01-01T18:59:13+09:00">2022-01-01</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Updated at: 2022-03-14 15:45:35" itemprop="dateModified" datetime="2022-03-14T15:45:35+09:00">2022-03-14</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/GAN/" itemprop="url" rel="index"><span itemprop="name">GAN</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <hr>
<ul>
<li>이번 포스팅은 티스토리에서 깃블로그로 이사 후 티스토리에 정리했던 내용에서 내용을 추가해서 작성한 글 입니다. 이전 글은 <a target="_blank" rel="noopener" href="https://sensibilityit.tistory.com/522">Tistory</a>에서 보실 수 있습니다.</li>
</ul>
<hr>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><hr>
<p><strong>Unsupervised image-to-image translation개념</strong><br>Unsupervised image-to-image translation tasks는 unpaired train data에서 source domain X와 target domain Y간의 mapping이 되는 지점을 찾는 것을 목표로 하는 task 입니다.</p>
<p><strong>CUT(Contrastive Learning for unpaired image-to-image Translation)</strong><br>Contrastive Learning for unpaired image-to-image Translation은 두개의 도메인 (X, Y) 모두에 대해 하나의 Encoder만 사용하여 입력, 출력 패치(patch)의 mutual information을 최대화 하여 unsupervised image-to-image translation을 모델링 하는 SOTA 결과를 제공합니다.</p>
<p><strong>제안한 방법</strong><br>본 논문에서는 unpaired data간의 효율적인 매핑을 위해 contrastive learning, dual learning setting에 기반한 새로운 학습 방법을 제안합니다.</p>
<p><strong>mode collapse 문제 해결</strong><br>cycle consistency loss의 문제점들을 해결 하기 위해 self-supervised representation learning 분야에서 multiple views of the data 간의 contrastive learning을 이용한 CUT가 SOTA을 달성했지만 mode collapse 문제가 발생 합니다.</p>
<p>🗣 DCLGAN은 데이터 도메인에 따라 mode collapse가 발생할 수 있다는 문제점이 존재하여 이를 해결하기 위해 DCLGAN의 변형인 SimDCL도 논문에서 소개를 하고 있습니다. </p>
<p>이 논문에서 제안한 방법으로 mode collapse문제를 효율적으로 해결합니다.</p>
<ul>
<li>CUT: 1개의 Encoder 사용, 데이터에 따라 mode collapse 문제 발생</li>
<li>DCLGAN: 2개의 Encoder 사용, mode collapse 문제를 효율적으로 해결</li>
</ul>
<p><strong>결론</strong><br>image-to-image translation tasks에서 extensive ablation study을 다른 네트워크들에 비해 본 논문에서 제안한 접근 방식이 효과적이라는 것을 본 논문에서 입증하고 있습니다.</p>
<p>끝으로, unsupervised learning과 supervised learning 방법 사이의 격차를 효율적으로 줄일 수 있음을 보여주고 있습니다.</p>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><hr>
<p>image-to-image translation task는 이미지를 한 도메인에서 다른 도메인으로 변환하는 것을 목표로 합니다. 가장 일반적으로 사용하고 있는 방법은 GAN을 기반으로 하는 방법입니다.</p>
<p>GAN이 발전하게 된 이유중 하나는 adversarial loss를 사용한점 입니다. 하지만 adversarial loss를 unpaired unsupervised image-to-image translation에 사용하여 발생되는 문제점은 adversarial loss가 underconstrained 하다는 점 입니다.</p>
<p>🗣 <strong><em>adversarial loss가 underconstrained 하다면?</em></strong><br>💡 <em>두 도메인(X, Y) 사이에 여러개의 매핑이 존재하게 되어 네트워크 학습이 불안정하게 이루어집니다.</em></p>
<p>그래서 위와 같은 문제를 해결하기 위해 unpaired unsupervised image-to-image translation 에서는 cycle consistency을 사용하여 adversarial loss만 사용했을때 발생되는 문제점을 해결했습니다.</p>
<h2 id="Cycle-consistency"><a href="#Cycle-consistency" class="headerlink" title="Cycle consistency"></a>Cycle consistency</h2><p>cycle consistency loss는 *(그림 1)*처럼target Domain ➡️ source Domain으로 역방향 매핑을 학습할 때 사용하는 loss 입니다.</p>
<p>🗣 <strong><em>역방향 매핑을 학습한다는 의미는?</em></strong><br>💡 <em>입력된 이미지와 재구성된 이미지가 얼마나 동일하게 만들어 지는지를 학습하는 의미이며 cycle consistency loss가 그 차이를 측정하며 학습을 하게 됩니다.</em></p>
<p><img src="/images/post_images/post0006/figure0001.png"> <center><em>(그림 1). Cycle consistency loss</em></center></p>
<p>하지만 cycle consistency을 사용할 때 약간의 가정과 제약이 있습니다.</p>
<p>📌 <strong>cycle consistency의 가정</strong></p>
<ul>
<li>변환된 이미지는 target domain과 유사한 texture information을 가지므로 geometry 변경이 불가하다는 것을 가정으로 두고 cycle consistency를 사용합니다.</li>
<li>두 도메인 (X, Y) 간의 관계가 bijection 되도록 즉, 1:1 대응이 되도록 합니다.</li>
</ul>
<p>📌 <strong>cycle consistency의 제약</strong></p>
<ul>
<li>그로 인해 정확도 손실로 인해 재구성 과정이 제한되므로재구성 이미지의 diversity가 감소하게 됩니다.</li>
</ul>
<p>그래서 이러한 제약을 해결하기 위해 Contrastive Learning을 이용한 연구가 등장을 하게 되었습니다.</p>
<h2 id="Contrastive-Learning을-이용한-Image-to-Image-Translation"><a href="#Contrastive-Learning을-이용한-Image-to-Image-Translation" class="headerlink" title="Contrastive Learning을 이용한 Image-to-Image Translation"></a>Contrastive Learning을 이용한 Image-to-Image Translation</h2><p>cycle consistency loss는 재구성 이미지의 diversity가 감소하게 되는 등의 제약들이 발견되었습니다. 따라서 이를 해결하기 위해self-supervised representation learning 분야에서 multiple views of the data 간의 contrastive learning 방법이 이루어 졌습니다.</p>
<p>이 방법이 기존 cycle consistency loss을 사용한 방법의 제약을 효율적으로 해소함을 보여줌과 동시에 SOTA를 달성하게 되었습니다.</p>
<p>가장 최근에 나온 논문 중 대표적인 논문으로<code>CUT(Contrastive Learning for Unpaired Image-to-Image Translation)</code>이 있습니다. CUT는 <code>unpaired image-to-image translation taks에서 contrastive learning 방식이 효율적</code>이라는 것을 보여주었으며, <code>patch-based multi-layer PatchNCE loss</code>을 사용하여 <code>unpaired image-to-image translation을 위한 Contrastive Learning을 도입하여 입력 및 출력 이미지의 패치간의 mutual information을 최대화 하는 방향으로 학습</code>이 이루어지게 됩니다.</p>
<p>하지만 SOTA를 찍은CUT도 문제점이 보였습니다. 그 문제점은 바로 아이디어는 좋았지만 <code>두 도메인 (X, Y) 사이의 domain gap을 효율적으로 포착하지 못하여 충분히 성능을 끌어올리지 못하고 있다는 점</code> 입니다.</p>
<p>🗣 <strong><em>그럼, 왜 성능을 충분히 끌어 올리지 못했을까요?</em></strong><br>💡 <em>그 이유는 바로 CUT에서 사용한 아키텍처의 특정 부분의 디자인을 잘못 선택해서 성능이 떨어지게 되었습니다.</em></p>
<p>domain gap을 효율적으로 포착하기 위해서는 도메인 수 만큼 임베딩이 사용되어야 합니다. 하지만 CUT는 하나의 임베딩이 사용되어 성능을 제한하고 있다고 본 논문에서 주장을 하고 있습니다.</p>
<p>그래서 본 논문에서는 cycle-consistency의 제약을 피하고, domain gap을 효율적으로 포착할 수 있도록 한개 이상의 임베딩을 사용하고, contrastive learning 방법을 더욱 활용한 DCLGAN을 제안합니다.</p>
<h2 id="DCLGAN"><a href="#DCLGAN" class="headerlink" title="DCLGAN"></a>DCLGAN</h2><hr>
<p><img src="/images/post_images/post0006/figure0002.gif"> <center><em>(그림 2). Contrastive Learning for Unpaired Image-to-Image Translation(CUT) 논문의 Patch-wise Contrastive Learning for one-sided translation</em></center></p>
<p>📌 <strong>목표</strong><br>DCLGAN의 목표는 별도의 임베딩을 사용하여 입력 및 출력 이미지 패치의 상관관계를 학습하여 mutual information을 극대화 하는 것을 목표로 합니다.</p>
<p>📌 <strong>학습 방법</strong><br>DCLGAN은 CUT의 성능을 제한시킨 디자인인 1개의 임베딩을 사용하는 점을 개선시켜 서로 다른 도메인에 서로 다른 Encoder 및 projection heads를 사용함으로써 두 도메인 간의 접점이 되는 부분을 극대화 시키기 위한 1개 이상의 임베딩을 학습합니다.</p>
<p>📌 <strong>발견한점</strong></p>
<ul>
<li>CUT과 달리 DCLGAN의 학습 방법은 dual learning 방식 입니다. 이 방식이 오히려 학습을 안정화 시키는데 도움이 된다고 합니다.</li>
<li>또한 CUT에서 사용한 PatchNCE loss에서 RGB pixel을 제거하는 것이 학습하는대에 있어 도움이 될 수 있음을 발견했습니다.</li>
<li>geometrical structure에 대한 제약이 없는 경우에는 cycle-consistency가 불필요 하다는 점도 발견했습니다.</li>
<li>DCLGAN은 데이터 도메인에 따라 mode collapse가 발생될 수 있지만 그 변형인 SimDCL은 mode collapse를 방지하는데 효과적인 점을 발견했습니다.</li>
</ul>
<p><img src="/images/post_images/post0006/figure0003.png"> <center><em>(그림 3). DCLGAN architecture</em></center></p>
<h2 id="정리"><a href="#정리" class="headerlink" title="정리"></a>정리</h2><hr>
<p>본 눈문은 CycleGAN과 CUT의 한계를 극복할 수 있는 새로움 프레임 워크와 변형 네트워크를 제시하고 있습니다.</p>
<h3 id="CycleGAN"><a href="#CycleGAN" class="headerlink" title="CycleGAN"></a>CycleGAN</h3><p>cycle consistency으로 발생하는 단점.</p>
<h3 id="CUT"><a href="#CUT" class="headerlink" title="CUT"></a>CUT</h3><p>contrastive learning의 효율성을 보여주었지만 한개의 임베딩을 사용해서 domain gap을 효율적으로 포착하지 못할 수 있다는 한계점.<br>또한 여러가지 다양한 실험을 통해 SOTA에 비해 본 논문에서 제안한 방식이 훨씬 효과적이라는 점을 입증하게 되었으며, self-supervised learning 분야에서 contrastive learning 방법이 그랬던것 처럼 unsupervised and supervised learning 방법 사이의 격차를 성공적으로 좁힐 수 있다는 점을 보여주고 있습니다.</p>
<h1 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h1><hr>
<h2 id="Supervised-methods"><a href="#Supervised-methods" class="headerlink" title="Supervised methods"></a>Supervised methods</h2><p>📌 관련 논문: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1611.07004">Pix2Pix</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1711.11585.pdf">Pix2PixHD</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.07291">SPADE</a></p>
<h3 id="Pix2Pix"><a href="#Pix2Pix" class="headerlink" title="Pix2Pix"></a>Pix2Pix</h3><p>general methods만 사용하여 여러개의 image-to-image translation tasks을 지원하는 작업에 구애받지 않는 image translation을 처음 수행한 논문 입니다.</p>
<h3 id="Pix2PixHD"><a href="#Pix2PixHD" class="headerlink" title="Pix2PixHD"></a>Pix2PixHD</h3><p>기존 Pix2Pix 논문에서 확장된 방법으로, 고해상도 이미지를 합성할 수 있는 방법 입니다.</p>
<h3 id="SPADE"><a href="#SPADE" class="headerlink" title="SPADE"></a>SPADE</h3><p>생성된 이미지의 품질을 더욱 향상 시키기 위해 spatially-adaptive normalization layer을 도입한 논문입니다.<br>😱 단점: supervised 접근 방법은 학습을 위해 paired data가 필요합니다.</p>
<h2 id="Unsupervised-methods"><a href="#Unsupervised-methods" class="headerlink" title="Unsupervised methods"></a>Unsupervised methods</h2><p>📌 관련 논문: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1804.04732">MUNIT</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1808.00948">DRIT</a>, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1711.09020">StarGAN</a><br>비지도 학습 방법은 주로 shared latent space, cycle-consistency assumption을 가정을 두고 있습니다.</p>
<h3 id="MUNIT"><a href="#MUNIT" class="headerlink" title="MUNIT"></a>MUNIT</h3><p>latent space을 style code &amp; content code로 분리하여 domain-specific features를 분리하는 특징이 있는 논문 입니다.</p>
<h3 id="DRIT"><a href="#DRIT" class="headerlink" title="DRIT"></a>DRIT</h3><p>domain-specific attribute space and shared information을 포착하는 content space을 포함한 두 space에서 이미지 임베딩을 하는 논문 입니다.</p>
<h3 id="StarGAN"><a href="#StarGAN" class="headerlink" title="StarGAN"></a>StarGAN</h3><p>대표적인 multi domain image-to-image translation 으로, unified model architecture을 사용하여 여러 도메인에서 이미지를 translation 하는 논문 입니다.</p>
<h2 id="Break-the-cycle"><a href="#Break-the-cycle" class="headerlink" title="Break the cycle"></a>Break the cycle</h2><p>📌 관련 논문: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1703.10593">CycleGAN</a>, <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Nizan_Breaking_the_Cycle_-_Colleagues_Are_All_You_Need_CVPR_2020_paper.pdf">CouncilGAN</a>, <a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/2017/file/59b90e1005a220e2ebc542eb9d950b1e-Paper.pdf">DistanceGAN</a>, <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Fu_Geometry-Consistent_Generative_Adversarial_Networks_for_One-Sided_Unsupervised_Domain_Mapping_CVPR_2019_paper.pdf">GCGAN</a></p>
<h3 id="CycleGAN-1"><a href="#CycleGAN-1" class="headerlink" title="CycleGAN"></a>CycleGAN</h3><p>대표적인 unpaired data image-to-image translation으로 cycle-consistency loss을 사용하여 adversairal loss의 문제인 mode collapse 단점을 극복하기 위한 네트워크로, 입력 이미지를 target domain으로 변환하고 입력 및 생성된 이미지의 정확도를 유지하며 두개의 매핑을 동시에 학습하는 네트워크 입니다.</p>
<p>하지만 cycle-consistency의 문제를 완화하기 위해 break the cycle을 시도하고 있으며 대표적인 네트워크가 CUT 입니다.</p>
<h3 id="CouncilGAN"><a href="#CouncilGAN" class="headerlink" title="CouncilGAN"></a>CouncilGAN</h3><p>council loss와 함께 두개 이상의 Generator, Discriminator을 사용합니다.</p>
<p><code>본 논문에서는 CycleGAN, CUT의 장점을 모두 활용합니다. 특히 mutual information maximization을 통해 cycleGAN 아키텍처를 기반으로 한 양방향 unsupervised image-to-image translation이 가능하도록 합니다.</code></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="ko">
    <link itemprop="mainEntityOfPage" href="http://mulkong.github.io/2021/12/26/2021%E1%84%82%E1%85%A7%E1%86%AB-%E1%84%92%E1%85%AC%E1%84%80%E1%85%A9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="mulkong">
      <meta itemprop="description" content="공부한 내용을 바탕으로 정리한 기술블로그 입니다.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mulkong DeepLearning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/26/2021%E1%84%82%E1%85%A7%E1%86%AB-%E1%84%92%E1%85%AC%E1%84%80%E1%85%A9/" class="post-title-link" itemprop="url">2021년 회고</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">작성일</span>
      

      <time title="Post created: 2021-12-26 23:08:24 / Updated at: 23:31:50" itemprop="dateCreated datePublished" datetime="2021-12-26T23:08:24+09:00">2021-12-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Diary/" itemprop="url" rel="index"><span itemprop="name">Diary</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="ko">
    <link itemprop="mainEntityOfPage" href="http://mulkong.github.io/2021/12/26/MemAE-Memorizing-Normality-to-Detect-Anomaly-Memory-augmented-Deep-Autoencoder-for-Unsupervised-Anomaly-Detection/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="mulkong">
      <meta itemprop="description" content="공부한 내용을 바탕으로 정리한 기술블로그 입니다.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mulkong DeepLearning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/26/MemAE-Memorizing-Normality-to-Detect-Anomaly-Memory-augmented-Deep-Autoencoder-for-Unsupervised-Anomaly-Detection/" class="post-title-link" itemprop="url">Memory-augmented Deep Autoencoder for Unsupervised Anomaly Detection [MemAE]</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">작성일</span>
      

      <time title="Post created: 2021-12-26 19:10:20 / Updated at: 19:35:42" itemprop="dateCreated datePublished" datetime="2021-12-26T19:10:20+09:00">2021-12-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/GAN/" itemprop="url" rel="index"><span itemprop="name">GAN</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <hr>
<ul>
<li>이번 포스팅은 티스토리에서 깃블로그로 이사 후 티스토리에 정리했던 내용에서 내용을 추가해서 작성한 글 입니다. 이전 글은 <a target="_blank" rel="noopener" href="https://sensibilityit.tistory.com/519">Tistory</a>에서 보실 수 있습니다.</li>
</ul>
<hr>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><hr>
<p>정상 데이터를 이용하여 Autoencoder(AE)을 학습하면 정상보단 비정상 데이터에 대해 더 높은 재구성 오류(reconstruction error)을 얻게 됩니다. 하지만 AE는 일반화(Generalization)가 잘 이루어진다는 특징이 있어서 비정상 데이터가 입력되어도 정상을 재구성 해야하지만 결함이 있는 부분까지 포함하여 재구성 하는 경우가 발생하게 됩니다.</p>
<p>이런 AE기반 이상 탐지(Anomaly Detection)의 한계점을 개선하기 위한 해결책으로 메모리 모듈(memory module)을 사용하여 AE을 augmented 하는 방법인 MemAE을 이 논문에서는 제안하고 있습니다.</p>
<p>방법은 아래와 같습니다.</p>
<ul>
<li>입력 $x$가 주어지면 MemAE는 먼저 Encoder을 통해 인코딩된 $z$을 얻습니다.</li>
<li>그 다음 입력된 이미지 중 메모리 모듈에서 정상 패턴인 부분에 해당하는 항목을 검색한 후</li>
<li>이를 query로 사용하여 $z$을 얻고</li>
<li>이를 Decoder을 통해 재구성 하는 방법입니다</li>
</ul>
<p>학습 단계에서는 정상 데이터에 대한 메모리 내용(memory content)을 메모리 모듈에 기록하도록 학습이 진행됩니다.</p>
<p>테스트 단계에서는 학습된 메모리의 weight는 더이상 업데이트 되지 않도록 고정되고 테스트할 query가 주어지면 정상 데이터에 대한 memory record을 기반으로 재구성이 이루어집니다.</p>
<hr>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><hr>
<h2 id="AutoEncoder을-이용한-Anomaly-Detection의-한계점"><a href="#AutoEncoder을-이용한-Anomaly-Detection의-한계점" class="headerlink" title="AutoEncoder을 이용한 Anomaly Detection의 한계점"></a>AutoEncoder을 이용한 Anomaly Detection의 한계점</h2><p>이상 탐지(anomaly detection)에서 AE는 정상 데이터의 reconstruction loss가 최소화 되도록 학습하며 테스트 과정 때 입력된 정상 데이터는 reconstruction loss가 낮고 비정상 데이터는 reconstruction loss가 커진다 라는 개념을 가정합니다.</p>
<p>하지만 이런 가정은 모든 상황에서 무조건 유효하지 않을 수 있습니다. 왜냐하면 AE는 general하게 학습되는 경우가 일반적이며 입력된 데이터를 그대로 복원하려는 성질을 갖고 있어서 비정상 데이터가 입력 되면 그대로 결함이 있는 상태로 재구성 하려는 성질을 갖고 있습니다.</p>
<p>그래서 소개하고 있는 MemAE 논문에서는 결함이 포함된 상태로 재구성 하는 문제점을 해결하기 위해 정상 데이터를 인코딩할 때 정상 데이터에 대한 메모리를 얻은 후 이를 기반으로 해서 정상 데이터를 생성하는 메모리 모듈(Memory Module)을 추가한 MemAE(Memory-augmen ted Deep Autoencoder)을 제안하고 있습니다.</p>
<h2 id="Memory-Module"><a href="#Memory-Module" class="headerlink" title="Memory Module"></a>Memory Module</h2><p>인코딩된 vector &amp;z&amp;을 Decoder에 직접 전달하지 않고 아래 그림과 같이 Memory module에서 입력 데이터를 기반으로 가장 관련성이 높은 메모리 항목을 검색한 후 &amp;z&amp;을 query로 사용하여 Decoder에 전달 합니다.</p>
<img src="/images/post_images/post0004/figure_0002.png" width="100%" height="100%">


<hr>
<h1 id="Memory-augmented-Autoencoder"><a href="#Memory-augmented-Autoencoder" class="headerlink" title="Memory-augmented Autoencoder"></a>Memory-augmented Autoencoder</h1><hr>
<img src="/images/post_images/post0005/figure0001.png" width="100%" height="100%">

<h2 id="Memory-module-with-Attention-based-Sparse-Addressing"><a href="#Memory-module-with-Attention-based-Sparse-Addressing" class="headerlink" title="Memory module with Attention-based Sparse Addressing"></a>Memory module with Attention-based Sparse Addressing</h2><p>이 논문에서 제안한 Memory Module의 구성 요소를 정리 해보자면 다음과 같습니다.</p>
<ul>
<li>soft addressing vectors $w$<ul>
<li>입력된 데이터의 패턴을 Memory Address에 기록하는 vectors</li>
</ul>
</li>
<li>attention-based addressing operator<ul>
<li>데이터의 패턴이 기록된 Memory Address에 접근하기 위한 연산자</li>
</ul>
</li>
</ul>
<h3 id="Memory-baed-Representation"><a href="#Memory-baed-Representation" class="headerlink" title="Memory-baed Representation"></a>Memory-baed Representation</h3><p>입력 데이터를 기반으로 인코딩된 query $ \mathbb{Z} \in \mathbb{R}^C $가 주어지면 입력 데이터의 패턴을 Memory Address에 기록하기 위해 soft addressing vectors $ \mathbf{w} \in \mathbb{R}^{1 \times N}$을 얻습니다.</p>
<p>그 다음 Memory $\mathbf {M} $에 접근한 후 $\hat{\mathbf{z}}$을 얻게 됩니다. 이 과정을 식으로 표현하면 다음 수식과 같이 표현 됩니다.</p>
<p>$$ \hat{\mathbb{z}} = \mathbb{w}\mathbf{M} = \sum_{i=1}^N {w_i,m_i,} $$</p>
<p>이 논문에서는 단순 인코딩된 vector $\mathbf{z}$을 바로 디코더에 넣는게 아니라 위와 같은 과정을 거친 후 얻어진</p>
<p>$\mathbf{\hat{z}}$을을 디코더에 입력하여 정상 패턴만 갖고 있는 데이터를 재구성 해야 하는게 최종 목표입니다. 그러면 세부적으로 어떤 과정을 거쳐가며 위와 같은 과정이 이루어지는지 살펴보도록 하겠습니다.</p>
<h3 id="Attention-for-Memory-Addressing"><a href="#Attention-for-Memory-Addressing" class="headerlink" title="Attention for Memory Addressing"></a>Attention for Memory Addressing</h3><p>메모리 모듈 $\mathbf{M}$은 학습 데이터의 정상 패턴(normal pattern)을 기록하도록 이루어져 있습니다. 정상 패턴을 기록하는 방법은 위에서 soft addressing vectors $\mathbf{w}$로 한다고 설명 드렸습니다. 이 과정을 좀 더 세분화시켜 정리를 하면 다음과 같이 정리를 할 수 있습니다.</p>
<ul>
<li><p>데이터를 인코더에 입력으로 주어 인코딩된 vector $\mathbf{z}$을 얻습니다.</p>
</li>
<li><p>$\mathbf{z}$을 memory addressing 체계를 사용하여 정상 패턴을 기록합니다.</p>
</li>
<li><p>그 기록된 공간을 content addressable memory로 정의합니다.</p>
</li>
<li><p>softmax 연산을 통해 데이터 패턴을 기록하는 weight $w_i$을 계산합니다.</p>
</li>
</ul>
<p>이 과정을 수식으로 나타내면 다은과 같이 정의됩니다.</p>
<p>$$ w_i = \frac{\exp{(d(\mathbf{z}, \mathbf{m_i}))}}{\sum_{j=1}^N {\exp (d(\mathbf{z}, \mathbf{m_j}))} }$$</p>
<p>정상 패턴을 기록하는 weight $w_i$을 얻는 수식에서 $d(.,.)$형태로 된 수식은 similarity measurement을 나타내며 이 논문에서는 cosin similarty로 정의했으며 수식은 다음과 같습니다.</p>
<p>$$d(\mathbf{z}, \mathbf{m_i}) = \frac{\mathbf{z} \mathbf{m_i^T}}{\parallel \mathbf{z} \parallel \parallel \mathbf{m_i} \parallel}$$</p>
<p>정상 패턴을 기록하는 부분은 아래 그림에 표시된 부분입니다.</p>
<img src="/images/post_images/post0005/figure_0002.png" width="100%" height="100%">

<p>위 과정을 거치므로써 메모리 모듈 $\mathbf{M}$은 $\mathbf{z}$와 가장 유사한 메모리 항목을 검색하여 $\mathbf{\hat{z}}$을 얻게 됩니다.</p>
<h3 id="적은-수의-memory-items-정상-데이터를-재구성-하다"><a href="#적은-수의-memory-items-정상-데이터를-재구성-하다" class="headerlink" title="적은 수의 memory items 정상 데이터를 재구성 하다"></a>적은 수의 memory items 정상 데이터를 재구성 하다</h3><p>위에서 소개한 방법 중 정상 패턴을 memory addressing 체계를 사용하여 기록하는 과정은 메모리 크기가 무한대로 큰 공간에 기록하는게 아니라 제한적인 메모리 크기를 갖도록 미리 제한을 걸어둡니다.</p>
<p>이렇게 메모리 크기를 제한해둔 상태면 그만큼 정상 패턴에 대한 items도 많이 부족해서 정상 데이터로 재구성할 수 없게 된다는 생각이 들 수 있습니다. 하지만 이 논문에서 소개하고 있는 <code>sparse addressing technique</code>을 사용해서 적은 수의 정상 패턴이 기록된 addressing memory items만으로도 효과적으로 Decoder을 통해 정상 데이터를 재구성할 수 있게 됩니다.</p>
<h3 id="attention-based-addressing의-한계점"><a href="#attention-based-addressing의-한계점" class="headerlink" title="attention-based addressing의 한계점"></a>attention-based addressing의 한계점</h3><p>attention-based addressing 방법을 사용하면 정상인 부분을 효과적으로 잘 재구성 하게 됩니다. 하지만 <code>세부적이고 미세한 결함인 부분들도 함께 재구성될 수 있게 됩니다.</code></p>
<p>그래서 이를 해결하기 위해 <code>hard shrinkage operation</code>을 사용하여 $\mathbf{w}$의 sparsity을 높입니다.</p>
<img src="/images/post_images/post0005/figure_0003.png" width="100%" height="100%">


<h2 id="Hard-Shrinkage-for-Sparse-Addressing"><a href="#Hard-Shrinkage-for-Sparse-Addressing" class="headerlink" title="Hard Shrinkage for Sparse Addressing"></a>Hard Shrinkage for Sparse Addressing</h2><p>Hard Shrinkage operation을 사용하는 이유는 attention-based addressing 까지만 이용한 상태의 weight $\mathbf{w}$을 이용해서 재구성을 하면 미세한 결함 부분들을 포함한 상태로 재구성이 된다는 한계점이 있었습니다.</p>
<p>그래서 이를 해결하고자 <code>hard shrinkage operation</code>을 적용하여 $\mathbf{w}$의 <code>sparsity을 높여서 한계점을 극복</code>할 수 있습니다.</p>
<p>shrinkage operation의 수식은 다음과 같습니다.</p>
<p>$$ \hat{w}_i = h(w_i ; \lambda) = \begin{cases} {w_i}, &amp; \mathbf{if} \ w_i &gt; \lambda , \ 0, &amp; \mathbf{otherwise}, \end{cases}$$</p>
<p>위 수식에서 $w_i$는 attention-based addressing과정 이후 $\hat{w}$을 지정하는 메모리의 i번째 항목을 나타내며 $\lambda$는 shrinkage threshold value을 나타내는 Hyper parameter입니다.</p>
<blockquote>
<p>실제로 코드를 구현할 때 threshold $\lambda$는 1/N ~ 3/N 간격으로 설정하면 최적의 결과를 얻을 수 있다고 논문에 나와있습니다.</p>
</blockquote>
<p>또한 식에서 discontinuous function의 backward을 구하는 것은 쉽지 않으므로 이 논문에서는 단순화를 위해 <code>$w$의 모든 항목이 음수가 아니라는 점을 고려</code>해서 <code>continuous ReLU activation function</code>을 사용하여 <code>Hard shrinkage을 재정의 하게 되었습니다.</code> 그 수식은 다음과 같습니다.</p>
<p>$$ \hat{w}_i = \frac{max(w_i - \lambda, 0) \cdot w_i} {|w_i - \lambda | + \epsilon} $$</p>
<p>위 수식에서 $ max( . , 0) $는 ReLU activation function 이므로 그 값은 매우 작은 scalar 입니다.</p>
<p>shrinkage후 $\hat{\mathbf{w}}$을 $\hat{w_i} = \hat{w_i} / \parallel \hat{\mathbf{w}}\parallel_1 $로 re-normalize 합니다. 그러면 latent representation $\hat{\mathbf{z}} = \hat{\mathbf{w}}\mathbf{M}$을 얻게 됩니다.</p>
<hr>
<h1 id="학습-방법"><a href="#학습-방법" class="headerlink" title="학습 방법"></a>학습 방법</h1><hr>
<h2 id="Reconstruction-loss"><a href="#Reconstruction-loss" class="headerlink" title="Reconstruction loss"></a>Reconstruction loss</h2><p>$$ R(\mathbf{x}^t, \mathbf{\hat{x}}^t) = \parallel \mathbf{x}^t - \mathbf{\hat{x}}^t \parallel _{2} ^2 , $$</p>
<h2 id="Entropy-loss"><a href="#Entropy-loss" class="headerlink" title="Entropy loss"></a>Entropy loss</h2><p>shrinkage operation 외에 학습 중 $\mathbf{\hat{w}}$에 대한 sparsity regulartizer을 최소화 합니다.<br>$$ E(\mathbf{\hat{w}}^t) = \sum_{i=1}^T {-w_i \cdot log(\hat{w}_i).}$$</p>
<h2 id="Total-loss"><a href="#Total-loss" class="headerlink" title="Total loss"></a>Total loss</h2><p>$$ L(\theta_e, \theta_d, \mathbf{M}) = \frac{1}{T} \sum_{t=1}^T (R(\mathbf{x}^t, \mathbf{\hat{x}}^t) + \alpha E(\mathbf{\hat{w}}^t)), $$</p>
<p>논문에서 실험해본 결과 $\alpha = 0.0002$가 가장 좋은 결과를 얻는다고 합니다. 또한 학습중 Memory module $\mathbf{M}$은 backpropagation &amp; Gradient Descent을 통한 최적화가 진행 되었습니다.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="ko">
    <link itemprop="mainEntityOfPage" href="http://mulkong.github.io/2021/12/25/GAN%E1%84%8B%E1%85%B3%E1%86%AF-%E1%84%8B%E1%85%B5%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%A1%E1%86%AB-%E1%84%92%E1%85%AD%E1%84%8B%E1%85%B2%E1%86%AF%E1%84%8C%E1%85%A5%E1%86%A8%E1%84%8B%E1%85%B5%E1%86%AB-Anomaly-Detection-%E1%84%87%E1%85%A1%E1%86%BC%E1%84%87%E1%85%A5%E1%86%B8-f-AnoGAN-vs-MemAE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="mulkong">
      <meta itemprop="description" content="공부한 내용을 바탕으로 정리한 기술블로그 입니다.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mulkong DeepLearning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/25/GAN%E1%84%8B%E1%85%B3%E1%86%AF-%E1%84%8B%E1%85%B5%E1%84%8B%E1%85%AD%E1%86%BC%E1%84%92%E1%85%A1%E1%86%AB-%E1%84%92%E1%85%AD%E1%84%8B%E1%85%B2%E1%86%AF%E1%84%8C%E1%85%A5%E1%86%A8%E1%84%8B%E1%85%B5%E1%86%AB-Anomaly-Detection-%E1%84%87%E1%85%A1%E1%86%BC%E1%84%87%E1%85%A5%E1%86%B8-f-AnoGAN-vs-MemAE/" class="post-title-link" itemprop="url">GAN을 이용한 효율적인 Anomaly Detection 방법 [f-AnoGAN vs MemAE]</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">작성일</span>

      <time title="Post created: 2021-12-25 13:18:02" itemprop="dateCreated datePublished" datetime="2021-12-25T13:18:02+09:00">2021-12-25</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Updated at: 2021-12-26 19:06:12" itemprop="dateModified" datetime="2021-12-26T19:06:12+09:00">2021-12-26</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/GAN/" itemprop="url" rel="index"><span itemprop="name">GAN</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <hr>
<ul>
<li>이번 포스팅은 티스토리에서 깃블로그로 이사 후 티스토리에 정리했던 내용에서 내용을 추가해서 작성한 글 입니다. 이전 글은 <a target="_blank" rel="noopener" href="https://sensibilityit.tistory.com/518">Tistory</a>에서 보실 수 있습니다.</li>
</ul>
<hr>
<p>Unsupervised Learning 방법으로 GAN을 이용한 Anomaly Detection 방법 중 Encoder 부분을 이용한 f-AnoGAN 방법이 있습니다. 이 방법은 효율적으로 Anomaly Detection을 잘 하지만 미세한 결함을 제대로 검출하기 힘들다는 한계점이 있습니다. 본 글에서는 f-AnoGAN의 특징들과 단점에 대해 간략히 소개를 하고 그 해결책에 대한 내용을 간략하게 정리해보기 위한 글 입니다.</p>
<p>비교를 할 논문은 아래와 같습니다.</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/abs/pii/S1361841518302640">f-AnoGAN:Fast unsupervised anomaly detection with generative adversarial networks</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.02639">MemAE:Memorizing Normality to Detect Anomaly: Memory-augmented Deep Autoencoder for Unsupervised Anomaly Detection</a></li>
</ul>
<hr>
<h1 id="f-AnoGAN"><a href="#f-AnoGAN" class="headerlink" title="f-AnoGAN"></a>f-AnoGAN</h1><p><strong>참고 링크</strong></p>
<ul>
<li>blog.promedius.ai/f-anogan-fast-unsupervised-anomaly-detection-with-gan/</li>
</ul>
<p><strong>Github tutorial</strong></p>
<ul>
<li>github.com/mulkong/f-AnoGAN_with_Pytorch</li>
</ul>
<p>f-AnoGAN의 학습 방법은 GAN 학습, Encoder 학습으로 총 2가지 step으로 이루어집니다.</p>
<img src="/images/post_images/post0004/figure_0001.png" width="100%" height="100%">

<hr>
<h2 id="GAN-학습"><a href="#GAN-학습" class="headerlink" title="GAN 학습"></a>GAN 학습</h2><p>정상 데이터로만 GAN 학습을 진행합니다. GAN이 잘 학습이 되었다면 정상 데이터에 대한 학습 분포를 기반으로 학습이 이루어져 정상 이미지만을 생성할 가능성이 높아집니다.</p>
<h2 id="Encoder-학습"><a href="#Encoder-학습" class="headerlink" title="Encoder 학습"></a>Encoder 학습</h2><p>GAN이 잘 학습 된 상태라면 이제 입력된 query data에 대해 Latent space mapping을 위한 Encoder 모델 학습을 진행합니다.</p>
<p>이와 같은 방식으로 진행한 이유는 query data(정상/비정상) 중 정상 데이터가 입력되면  Encoder을 통해서 정상 query data에 대해 latent space mapping된 Z값을 이용하여 Generator에 Z가 입력되게 됩니다. 그러면 Generator는 query data에 대한 정상 데이터를 생성하게 됩니다.</p>
<p>반면 비정상 query data가 입력으로 주어지면 Encoder는 query data에 대한 Feature을 추출해서 Z값으로 만든 후 비정상 query data의 Feature을 Generator에 입력으로 주어지면 query data을 기반으로 한 이미지가 생성됩니다.</p>
<p>GAN은 정상 이미지로만 학습이 진행되어 정상 이미지만을 생성하려고 할 것 입니다. 따라서 비정상 query data의 모양, 질감, 형태와 동일하지만 정상 상태로 이미지를 생성하게 되어 이상 탐지(anomaly detecTion)을 수행합니다.</p>
<h3 id="단점"><a href="#단점" class="headerlink" title="단점"></a>단점</h3><p>일반적으로 AutoEncoder을 이용한 Anomaly Detection을 진행하다보면 AutoEncoder의 특성상 일반화(Generalization)을 너무 잘 해서 정상만 생성하도록 해야하는데 비정상까지 재구성하게 되어 미세한 결함을 제대로 찾기 힘들다는 단점이 존재합니다.</p>
<h3 id="해결책"><a href="#해결책" class="headerlink" title="해결책"></a>해결책</h3><p>memory module을 사용하여 AutoEncoder을 augment(보강)하는 MemAE(memory-augmented autoencoder) 방법을 사용.</p>
<hr>
<h1 id="MemAE"><a href="#MemAE" class="headerlink" title="MemAE"></a>MemAE</h1><p>MemAE의 핵심 내용은 AutoEncoder가 너무 general하게 학습 되는 경우가 발생하여 정상 뿐만 아니라 간혹 비정상의 결함 부분 까지 포함하여 생성하게 된다는 단점이 존재하니 이를 해결하기 위해 정상 데이터을 Encoding할 때 정상 데이터에 대한 memory을 얻은 후 이를 기반으로 해서 정상 데이터를 생성하는 방법입니다.</p>
<img src="/images/post_images/post0004/figure_0002.png" width="100%" height="100%">

<p>학습 단계에서 memory는 정상 데이터에 가장 관련성이 높은 메모리 항목을 검색하며 데이터를 생성(재구성)하게 됩니다. 테스트 과정시 정상 데이터에 가장 관련성이 높은 메모리를 고정시킨 후 query data가 입력으로 주어지면 query data을 기반으로 한 정상 데이터를 메모리 기록으로 부터 검색을한 후 이를 기반으로 생성(재구성)이 이루어지게 됩니다.</p>
<hr>
<p>다음 포스팅할 글은 MemAE 논문에 대해 정리한 글을 작성하도록 하겠습니다.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="ko">
    <link itemprop="mainEntityOfPage" href="http://mulkong.github.io/2021/12/24/Improved%20anomaly%20detection%20by%20training%20an%20autoencoder%20with%20skip%20connections%20on%20images%20corrupted%20with%20stain%20shaped%20noise/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="mulkong">
      <meta itemprop="description" content="공부한 내용을 바탕으로 정리한 기술블로그 입니다.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mulkong DeepLearning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/24/Improved%20anomaly%20detection%20by%20training%20an%20autoencoder%20with%20skip%20connections%20on%20images%20corrupted%20with%20stain%20shaped%20noise/" class="post-title-link" itemprop="url">[Anomaly Detection] Improved anomaly detection by training an autoencoder with skip connections on images corrupted with Stain-shaped noise</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">작성일</span>
      

      <time title="Post created: 2021-12-24 21:44:10 / Updated at: 21:45:32" itemprop="dateCreated datePublished" datetime="2021-12-24T21:44:10+09:00">2021-12-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/GAN/" itemprop="url" rel="index"><span itemprop="name">GAN</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <hr>
<ul>
<li>이번 포스팅은 티스토리에서 깃블로그로 이사 후 티스토리에 정리했던 내용에서 내용을 추가해서 작성한 글 입니다. 이전 글은 <a target="_blank" rel="noopener" href="https://sensibilityit.tistory.com/517">Tistory</a>에서 보실 수 있습니다.</li>
<li>Paper 원문: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2008.12977">Improved anomaly detection by training an autoencoder with skip connections on images corrupted with Stain-shaped noise</a></li>
</ul>
<hr>
<h1 id="1-Abstract"><a href="#1-Abstract" class="headerlink" title="1.Abstract"></a>1.Abstract</h1><h3 id="산업-현장에서-AE를-활용한-Anomaly-Detection"><a href="#산업-현장에서-AE를-활용한-Anomaly-Detection" class="headerlink" title="산업 현장에서 AE를 활용한 Anomaly Detection"></a>산업 현장에서 AE를 활용한 Anomaly Detection</h3><p>Industrial vision에서 Anomaly Detection problems은 결함이 있거나 없는 arbitrary image를 clean image에서 mapping하도록 훈련된 AutoEncoder를 사용하여 해결할 수 있습니다.</p>
<h3 id="skip-connections이-있는-AutoEncoder-AES-를-사용한-이유"><a href="#skip-connections이-있는-AutoEncoder-AES-를-사용한-이유" class="headerlink" title="skip-connections이 있는 AutoEncoder(AES)를 사용한 이유"></a>skip-connections이 있는 AutoEncoder(AES)를 사용한 이유</h3><p>이 접근 방식에서 Anomaly Detection과정은 개념적으로 본다면 reconstruction residual 또는 reconstruction uncertainty에 의존합니다. 공통적으로 sharpness of the reconstruction를 높이기 위해 skip-connections이 있는 AutoEncoder를 고려하게 되었습니다.</p>
<h3 id="AES-Stain-noise-model-제안"><a href="#AES-Stain-noise-model-제안" class="headerlink" title="AES + Stain noise model 제안"></a>AES + Stain noise model 제안</h3><p>Reconstruction 과정 중 clean image가 나오게끔 하기 위해서 train image의 Clean image만 학습으로 진행하는 전략은, reconstruction을 담당하는 Network가 입력하는 대로 출력을 해버리는 identity mapping으로 수렴 되는것을 방지하기 위해 Noise model로 train image를 손상시키고 출력으로 Clean image가 나오도록 하는 목적으로 Stain noise model을 추가 할 것을 제안합니다.</p>
<h3 id="우리가-사용한-모델-짱짱👍-→-AES-Stain-model"><a href="#우리가-사용한-모델-짱짱👍-→-AES-Stain-model" class="headerlink" title="우리가 사용한 모델 짱짱👍 → AES+Stain model"></a>우리가 사용한 모델 짱짱👍 → AES+Stain model</h3><p>이 모델을 실제 결함 모양에 관계없이 임의의 실제 이미지에서 깨끗한 이미지를 재구성 하는 데 유리하다는 것을 보여줍니다.</p>
<h3 id="우리가-가라로-안했다는것을-증명하기-위해-이런-데이터셋으로-진행했다"><a href="#우리가-가라로-안했다는것을-증명하기-위해-이런-데이터셋으로-진행했다" class="headerlink" title="우리가 가라로 안했다는것을 증명하기 위해 이런 데이터셋으로 진행했다"></a>우리가 가라로 안했다는것을 증명하기 위해 이런 데이터셋으로 진행했다</h3><p>우리의 접근 방식의 관련성을 입증하는 것 외에도 우리의 검증은 pixel-wise및 image-wise anomaly detection을 위해 MVTec AD dataset에 대한 성능을 비교하여 reconstruction-based 방법에 대한 일관된 평가를 제공합니다.</p>
<hr>
<h1 id="2-Methods"><a href="#2-Methods" class="headerlink" title="2.Methods"></a>2.Methods</h1><h3 id="Anomaly-Detection-개요"><a href="#Anomaly-Detection-개요" class="headerlink" title="Anomaly Detection 개요"></a>Anomaly Detection 개요</h3><p>Anomaly detection은 Clean(Normal) data의 Distribution에 속하지 않는 다른 데이터들을 식별하는 작업으로 정의할 수 있습니다.</p>
<h3 id="Supervised-Learning으로-하는-Anomaly-Detection의-문제점"><a href="#Supervised-Learning으로-하는-Anomaly-Detection의-문제점" class="headerlink" title="Supervised Learning으로 하는 Anomaly Detection의 문제점"></a>Supervised Learning으로 하는 Anomaly Detection의 문제점</h3><p>Clean(Normal) data에 비해 Defective(Abnormal) data를 수집하는 것은 한계가 있고 만들다 하더라도 Data Imbalance가 발생합니다.</p>
<p>본 논문은 Unsupervised Learning으로 Anomaly Detection을 해결하는 논문입니다. 다만 이전에 정리했었던 AnoGAN, f-AnoGAN와의 차이점은 Query image가 입력으로 들어오면 그것을 구조는 동일하지만 Normal image로 image generation해주는 방법으로 GAN을 사용했습니다. 그러나 이 논문은 GAN이 mode collapse의 문제점을 지적하고 image generation 해주는 대신 image reconstruction 해주는 AutoEncoder 구조를 사용했습니다.</p>
<p>🤔 <strong><em>제 생각이지만 해당 저자는 f-AnoGAN과 AnoGAN의 방법만 인용했지, GAN 모델을 수정한 노력은 없어 보였습니다. 그래서 High resolution을 잘 생성해주는 GAN 모델들을 이용하면 mode collapse 문제는 많이 없어질텐데…좀 아쉽네요</em></strong></p>
<hr>
<h1 id="3-Training-Step"><a href="#3-Training-Step" class="headerlink" title="3.Training Step"></a>3.Training Step</h1><img src="/images/post_images/post0003/figure_0001.png" width="100%" height="100%">

<p>위 이미지는 Unsupervised Learning 방법으로 Anomaly Detection 하는 전체 Architecture framework를 보여주고 있습니다.</p>
<h3 id="Blue-box"><a href="#Blue-box" class="headerlink" title="Blue box"></a>Blue box</h3><p>위 모델의 구조를 살펴보고 해석 해보도록 하겠습니다. 일단 Blue box를 보시면 GAN 같은 경우 Normal image를 generation 해주는 용도로 사용이 되지만, 여기서는 <code>Skip-connections를 사용한 AutoEncoder(AES) 모델을 사용해서 입력 이미지로 query image가 들어오면 그것을 어떤 이미지가 들어오든 Normal image로 reconstruction 해주는 방법</code>입니다. </p>
<p>Stain Model을 사용한 이유는 AutoEncoder는 입력 이미지가 들어오면 그것을 그대로 복원해주는 성질이 있습니다. 근데 Denoising AutoEncoder를 생각해보면 noise가 있는 입력 이미지가 들어오면 그것을 선명한 이미지로 reconstruction해주는 성질이 있어서 일반 AutoEncoder를 사용할때 보다 더 선명하게 reconstruction 해주는 경향이 있습니다. </p>
<p>마찬가지로 해당 논문에서는 얼룩 무늬를 추가해주는 Stain noise model을 이용해서 입력 이미지를 결함이 있는 이미지로 만들어줘서 다시 깨끗한 이미지로 복원될 수 있도록 하는 역할을 가지고 있습니다. 즉, Identity mapping이 안되도록 방지해주기 위해 Stain noise model을 사용하는 것 입니다.</p>
<p>좀 더 자세히 모델을 살펴보도록 하겠습니다.</p>
<img src="/images/post_images/post0003/figure_0002.png" width="100%" height="100%">

<p>이 논문에서 사용한 데이터 크기는 256x256인 오직 Clean(Normal) Image로 학습되었습니다. 여기서 위 이미지(AE skip-connection architecture)처럼 <code>bottleneck 구조를 갖는 Skip-connection 을 사용함으로 써 model에 projection한 임의의 이미지로부터 좀 더 선명한 이미지가 reconstruction</code> 된다고 합니다.</p>
<p>AESc(AE skip-connection) 모델 구조를 보면 U-Net과 모양이 비슷하다는 생각이 자연스럽게 들게 됩니다. 그래서 한번 U-Net과 AESc를 비교 해보았습니다.</p>
<h2 id="3-1-AESc-vs-U-Net"><a href="#3-1-AESc-vs-U-Net" class="headerlink" title="3-1. AESc vs U-Net"></a>3-1. AESc vs U-Net</h2><img src="/images/post_images/post0003/figure_0003.png" width="100%" height="100%">

<h3 id="Autoencoder"><a href="#Autoencoder" class="headerlink" title="Autoencoder"></a>Autoencoder</h3><p>Autoencoder는 <code>입력 이미지에서 핵심 Feature들만 뽑는 구조</code>이며 Bottleneck을 갖고 있습니다. Bottleneck 구조를 갖는 모델은 Image-to-Image translation tasks에서 많이 사용되는 구조이며 <code>결과 이미지의 형태들이 급진적으로 변화하는 특징</code>을 갖고있습니다.</p>
<h5 id="Bottleneck-구조를-갖는-AutoEncoder-구조의-장단점"><a href="#Bottleneck-구조를-갖는-AutoEncoder-구조의-장단점" class="headerlink" title="Bottleneck 구조를 갖는 AutoEncoder 구조의 장단점"></a>Bottleneck 구조를 갖는 AutoEncoder 구조의 장단점</h5><ul>
<li>장점: Bottleneck 구조는 갖고 있어서 핵심 Feature를 추출해낼 수 있으며 Domain이 다른 이미지로 변형을 하고 싶을 때 적합하다.</li>
<li>단점: 생성(복원)된 이미지의 Detail이 떨어진다.</li>
</ul>
<h5 id="Bottleneck-구조를-갖는-AutoEncoder를-사용한-이유"><a href="#Bottleneck-구조를-갖는-AutoEncoder를-사용한-이유" class="headerlink" title="Bottleneck 구조를 갖는 AutoEncoder를 사용한 이유"></a>Bottleneck 구조를 갖는 AutoEncoder를 사용한 이유</h5><p>추가적으로 Bottleneck 구조를 갖는 AutoEncoder를 사용한 이유는 Anomaly Detection tasks에서 defective structures를 image distribution에서 제외하는 것은 반복적으로 발생하는 문제라고 합니다. 그래서 Bottleneck 구조를 갖는 AE로 Feature map을 압축하는 과정은 Normal Image를 Manifold에 놓여지도록 reconstruction을 일반화 합니다.</p>
<h3 id="Unet"><a href="#Unet" class="headerlink" title="Unet"></a>Unet</h3><p>U-Net 구조의 특징은 “Skip-connection이 잇다보니 입력된 영상에 대한 detail들이 마지막 layer 까지 잘 전달 된다는 특징”이 있습니다. 그래서 아무래도 AutoEncoder의 결과와 비교해보면 output image quality가 더 좋습니다. 그렇지만 단점도 존재합니다. “Skip-connection은 depth가 거의 없다보니 depth가 어느정도 있는 다른 네트워크 구조에 비해 생성된 결과가 별로”라는 점 입니다. </p>
<h5 id="U-Net-구조의-특징과-장단점"><a href="#U-Net-구조의-특징과-장단점" class="headerlink" title="U-Net 구조의 특징과 장단점"></a>U-Net 구조의 특징과 장단점</h5><ul>
<li>특징: paired된 dataset이 어느정도 비슷한 컨텐츠들이 있는 경우 skip-connection을 많이 사용하는 경향을 보이고 있다.</li>
<li>장점: 처음 detail들이 마지막 layer까지 잘 전달 된다.</li>
<li>단점: skip-connection을 사용해서 depth가 거의 없다.</li>
</ul>
<h3 id="Bottleneck-구조를-갖는-AutoEncoder에-Skip-connection을-사용한-이유"><a href="#Bottleneck-구조를-갖는-AutoEncoder에-Skip-connection을-사용한-이유" class="headerlink" title="Bottleneck 구조를 갖는 AutoEncoder에 Skip-connection을 사용한 이유"></a>Bottleneck 구조를 갖는 AutoEncoder에 Skip-connection을 사용한 이유</h3><p>AE skip-connection architecture와 같이 Skip-connections을 사용하면 처음 detail들이 마지막 layer까지 전달되다 보니 reconstruction image가 보다 더 선명해 진다고 합니다. 근데 <code>U-Net에서 Skip-connection을 사용할때는 Decoder 구조에 feature map을 Concatenation</code>해서 사용했지만 해당 논문에선 <code>encoder에서 decoder로 feature map을 Addition</code> 시켜주었다고 합니다.</p>
<p>red box에서 다시 설명을 하겠지만, Skip-connection이 없는 그냥 AE를 사용해서 얻은 reconstruction image를 query image랑 MSE로 loss를 구하면 reconstruction image가 나중에는 결국 blurry한 image로 reconstruction됩니다. 또한 reconstruction loss가 커지게 되어서 엽력 이미지랑 모양이 똑같은 이미지로 복원이 잘 안이루어지게 됩니다.</p>
<h3 id="GAN-말고-AESc를-사용한-이유"><a href="#GAN-말고-AESc를-사용한-이유" class="headerlink" title="GAN 말고 AESc를 사용한 이유"></a>GAN 말고 AESc를 사용한 이유</h3><p>위에서도 언급한 내용으로 본 논문에서는 Clean(Normal) image를 복원 시키는 과정에서 GAN의 mode collapse 문제점을 지적하며 GAN 말고 Skip-connection을 사용한 AutoEncoder(AECs) 구조를 사용했습니다.</p>
<p>Anomaly Detection은 Unsupervised Learning으로 접근하고 있으며 그중에 대표적인 방법 중 GAN을 사용 안하는 이유는 딱 3가지 입니다.</p>
<ul>
<li>mode collapse로 인해 학습 하기가 어렵다.</li>
<li>generative distribution에서 결합 샘플을 제외 못함 -&gt; 이 문제는 AE에서 Bottleneck 구조로 해결.</li>
<li>inference 과정시 query image의 distribution과 latent space에 속하는 가장 유하산 출력 이미지를 생성해야 하는 latent space 속의 latent vector를 찾기 위해서 추가적인 최적화 단계가 필요.</li>
</ul>
<p>비교를 위해 AnoGAN과 여기서 제안한 방법을 비교해보았는데 당연히(?) 이 논문에서 제안한 방법이 더 잘 나왔다 라는 결과가 나와서 GAN 사용 안하고 AES + Stain Model을 사용한 것 같습니다. </p>
<p><strong><em>🤔 개인적으로 여기서 의문이 드는 점은 AnoGAN의 GAN Architecture는 DCGAN으로 이루어져 있습니다. DCGAN은 mode collapse를 방지할 수 없어서 개선된 모델들이 더 많이 나왔습니다. 근데 굳이 64x64 이미지로 학습을 진행한 AnoGAN과 비교를  하다니…. 의문점이 많다는 생각이 들었습니다.</em></strong></p>
<img src="/images/post_images/post0003/figure_0004.png" width="100%" height="100%">

<p>위 결과 그림은 AE와 AESc 모델 각각 Stain noise model를 추가한 것과 추가하기 전 reconstruction image 결과 비교한 그림입니다. AutoEncoder와 Skip-connections를 사용한 AutoEncoder 모델 각각 Identity mapping을 방지하기 위해 넣어준 Stain noise model과 아무것도 안넣어준 경우의 reconstruction image의 결과를 비교한 그림입니다. <code>None의 경우 Identity mapping이 이루어져서 입력 이미지 그대로 출력되어 제대로 결측인 부분들은 감지하지 못합니다. 그러나 Stain noise model을 추가한 결과를 보면 Identity mapping을 방지해서 결함이 있는 이미지가 들어오더라도 구조는 동일하게 가지만 정상인 이미지로 reconstruction</code>하는 것을 볼 수 있습니다. 다만 <code>skip connections을 사용하지 않아서 AE기반인 모델들은 이미지가 blurry</code>하게 나오는 것을 볼 수 있습니다.</p>
<p>AESc를 보면  AE와는 다르게 image가 선명하게 복원되는 것을 볼 수 있으며  Stain model을 사용한 결과가 Identity mapping이 안이루어진 모습을 확인할 수 있습니다.</p>
<hr>
<p>위에서 설명한 Train 과정에서 있는 중요 내용들을 요약하면 다음과 같습니다.</p>
<ul>
<li><p>GAN의 mode collapse 문제로 AutoEncoder 구조를 사용해서 Normal Image를 reconstruction 하도록 진행.</p>
</li>
<li><p>AE는 Bottleneck 구조를 갖는다. 따라서 Feature map을 압축하는 과정이 Normal Image Manifold에 놓여지도록 reconstruction을 일반화 시킬 수 있다.</p>
</li>
<li><p>AutoEncoder만 사용하면 Identity mapping이 발생되므로 Stain noise model을 사용했다.</p>
</li>
<li><p>AutoEncoder에 Skip-connections을 추가하므로 써 reconstruction image가 blurry하게 복원 되는 것을 방지했으며 U-Net과의 차이점은 concatenation 말고 Addition 시켜주었다는 점이다</p>
</li>
</ul>
<hr>
<hr>
<h1 id="4-Test-Step"><a href="#4-Test-Step" class="headerlink" title="4.Test Step"></a>4.Test Step</h1><h3 id="Red-box"><a href="#Red-box" class="headerlink" title="Red box"></a>Red box</h3><p>red box에 있는 내용은 test data를 이용해서 inference하는 과정입니다. 여기서 눈여겨 볼 점은 inference할 때 두가지의 전략(<code>Residual-based detection과 Uncertainity-based detection</code>)으로 진행 했습니다.</p>
<h2 id="4-1-Residual-based-Detection"><a href="#4-1-Residual-based-Detection" class="headerlink" title="4-1. Residual-based Detection"></a>4-1. Residual-based Detection</h2><p>Test과정 중 Query image랑 Query image를 기반으로 Clean(Normal) image로 reconstruction된 image와 차이를 계산하는 것 입니다. 이때 사용되는 loss는 L2Norm(MSE)이고, Image-wise 관점과 Pixel-wise 관점에서 이루어 집니다.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">• residual(잔차)</span><br><span class="line">→  모집단에서 추출한 표본둘의 평균(표본평균)과 개별 표본갑 간의 &#39;편차&#39;를 말하지만 주로 &#39;추정오차 (Estimation Error)&#39;와 거의 같음 의미를 지닌다.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">• 추정 오차 (Estimation Error)</span><br><span class="line">→  표본 집단에 기초해 산출된 기대값(추정값)과 확률 시행 결과의 관측값 과의 차이.</span><br></pre></td></tr></table></figure>

<h3 id="Residual-baded-detection의-단점과-MCDropout-사용-이유"><a href="#Residual-baded-detection의-단점과-MCDropout-사용-이유" class="headerlink" title="Residual-baded detection의 단점과 MCDropout 사용 이유"></a>Residual-baded detection의 단점과 MCDropout 사용 이유</h3><p>Anomaly Detection 하고싶은 부분 말고 <code>나머지 뒷 배경과의 대조가 명확하지 않은 경우</code>. Normal Image로 Reconstruction을 해도 reconstruction loss가 충분하게 높지 않게 됩니다. 따라서 <code>MCDropout을 사용해서 prediction uncertainity을 정량화 하여 anomaly detection에 사용</code>합니다.</p>
<h2 id="4-2-Uncertainty-based-Detection"><a href="#4-2-Uncertainty-based-Detection" class="headerlink" title="4-2. Uncertainty-based Detection"></a>4-2. Uncertainty-based Detection</h2><p>Uncertainity-based detection 방법은 Bayesian Estimation에서 나온 개념입니다. <code>Uncertainty(불확실성)는 확률 변수의 분산 크기이며 확률 변수가 얼마나 random한지 측정하는 sclar값</code> 입니다. 그래서 해당 값은 Bayesian Model을 이용한 Estimation에서 확인할 수 있는데 Bayesian Model의 parameter 수가 많아서 model이 많이 무겁다고 합니다. 그래서 이와 비슷한 효과를 주는 방법에서 <code>Uncertainity를 정량화 하는 방법으로 MCDropout(Monte Carlo Dropout)을 사용해서 Uncertainity를 정량화</code> 한다고 합니다.</p>
<p>그래서 <code>MCDropout으로 추정한 30개의 output image 사이의 variance(분산)으로 추정할 때 훈련중에 볼 수 없는 structures, 즉, 이상징후가 더 높은 불확실성(uncertainties)과 상관성을 갖는 직관에 의존하는 방법</code>입니다. 그래서 해당 논문에서는 AutoEncoder의 layer가 깊어질수록 dropout level 을 증가하면서 [0, 0, 10, 20, 30, 40] 적용한 결과 더 정확한 검출이 얻어지는 것을 밝혀냈다고 합니다.</p>
<img src="/images/post_images/post0003/figure_0005.png" width="100%" height="100%">

<p>위 그림은 reconstruction residual가 대부분 uncertainty와 상관관계가 있음을 보여주는 그림 입니다. 첫번째, 두번째 행은 Stain noise model을 사용하지 않고 AE and AECs networks를 학습한 경우이고 세번째, 네번째 행은 Stain noise model을 사용한 경우입니다. 두가지 경우 공통적으로 test 할 때 MCDropout을 사용 안한 상태로 Residual-based detection과 Uncertainty-based detection한 결과를 나타내고 있습니다. </p>
<h3 id="데이터셋-상황에-따른-Detection-방법-선택"><a href="#데이터셋-상황에-따른-Detection-방법-선택" class="headerlink" title="데이터셋 상황에 따른 Detection 방법 선택"></a>데이터셋 상황에 따른 Detection 방법 선택</h3><p>Residual-based Detection은 Reconstruction error threshold를 넘어가면 결함으로 간주하는 원리입니다. AECs가 Normal 한 이미지를 reconstruction을 하도록 학습하는 과정은 결함이 있는 구조를 깨끗한 이미지로 대처하도록 하는 것이 목적이지 그 주변 환경과의 대비를 더 명확하게 주라고 학습을 한 것은 아닙니다. 그래서 reconstruction image가 주변 환경과 대조가 잘 되지 않으면 residual intensities가 낮게 나옵니다.</p>
<p>반면에 Uncertainity-based Detection은 이미지의 구조와 주변 환경의 대비에 의존하지 않는다는게 가장 큰 특징입니다. 따라서 대비가 늦은 결함 이미지의 경우 Anomaly Detection 기능이 향상됩니다.</p>
<p>정리를 한번 해보도록 하겠습니다.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">주변환경과 데이터셋 대비가 명확하다</span><br><span class="line">→  Residual-based Detection 전략이 좋다.</span><br><span class="line"></span><br><span class="line">주변 환경과 데이터셋 대비가 명확하지 않다.</span><br><span class="line">→  Uncertainity-based Detection 전략이 좋다.</span><br></pre></td></tr></table></figure>
<p>추가적으로 위에서도 언급했다 싶이 Residual-based Detection 전략으로 진행할 경우 AESc + Stain model은 일반적으로 산발적인 반점으로 구성 된 reconstruction residual을 유발하고 낮은 대비를 갖는 데이터셋에 대해 결함을 놓치게 됩니다. 이런 경우에는 Uncertainity-based 전략이 효과적입니다.</p>
<hr>
<h1 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5.Conclusion"></a>5.Conclusion</h1><p>Query image에서 Normal image를 reconstruction 하는 것을 기반으로 한 anomaly detection 방법을 진행하기 위해서 본 논문에서는 <code>Skip-connection을 사용한 AutoEncoder인 AESc를 기반으로 MCDropout으로 30번 estimated된 reconstruction residual 또는 prediction uncertainity에 의존하여 Anomaly Detection을 진행</code>합니다.</p>
<p>Skip-connections를 Addition 시켜 사용한 AutoEncoder 구조를 사용할 때 장접을 본 논문에서는 입증을 하였으며 <code>Identity mapping이 이루어지지 않도록 train image가 Stain Noise model로 corrupted 시켜서 학습을 진행</code>하였습니다.</p>
<p>또한 본 논문에서 사용된 새로운 접근 방법은 일반 AutoEncoder보다 상당히 잘 MVTec AD Dataset들의 결함들을 잘 검출 해냈으며 AECs + Stain noise model을 사용하여 AutoEncoder와의 Uncertainity-based Detection 전략을 공정하게 비교해냈습니다.</p>
<p>Reconstruction residual과 달리 Uncertainity Indicator는 결함과 그 주변 사이 환경의 대비와는 무관합니다. 따라서 주변 환경과 대비가 뚜렷하다면 Reconstruction residual 전략으로 가고 대비가 뚜렷한 대비가 없다는 Uncertainity 기반 전략으로 Anomaly Detection을 진행하면 됩니다.</p>
<p>또한 Residual-based detection 전략에 비해 Uncertainty-based detection 전략은 Normal한 query image에서 false-positive rate를 증가키시게 되는 단점이 존재합니다.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="ko">
    <link itemprop="mainEntityOfPage" href="http://mulkong.github.io/2021/05/05/F-AnoGAN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="mulkong">
      <meta itemprop="description" content="공부한 내용을 바탕으로 정리한 기술블로그 입니다.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mulkong DeepLearning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/05/F-AnoGAN/" class="post-title-link" itemprop="url">[F-AnoGAN]Fast unsupervised anomaly detection with generative adversarial networks</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">작성일</span>
      

      <time title="Post created: 2021-05-05 15:09:09 / Updated at: 17:29:50" itemprop="dateCreated datePublished" datetime="2021-05-05T15:09:09+09:00">2021-05-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/GAN/" itemprop="url" rel="index"><span itemprop="name">GAN</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <hr>
<ul>
<li>GAN을 사용한 최초 Anomaly Detection 방법인 AnoGAN의 후속 모델로 Encoder 모델을 사용하여 더 빠르게 $G(x)$와 $x$를 latent space안에 mapping 시켜 Anomaly Detection 하는 방법입니다.</li>
<li>Paper 원문: f-AnoGAN: <a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/abs/pii/S1361841518302640">Fast unsupervised anomaly detection with generative adversarial networks</a></li>
<li>f-AnoGAN tutorial code(Pytorch): <a target="_blank" rel="noopener" href="https://github.com/mulkong/f-AnoGAN_with_Pytorch">Tutorial Link</a></li>
</ul>
<hr>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><h2 id="문제점"><a href="#문제점" class="headerlink" title="문제점"></a>문제점</h2><p>정확한 annotation은 시간이 많이 들기 때문에 전문가(방사선사, 의사 등)가 clinical imaging(임상 영상)을 직접 annotation을 표시한 데이터를 얻는 것은 어렵다. 또한 모든 병변에 대해 annotation이 표시되지 않을 수 있으며 annotation에 대해서 정확하게 이 병변이 어떤 병변인지 설명 되어있지 않은 경우도 있습니다.</p>
<h2 id="지도-학습-supervised-learning-의-장단점"><a href="#지도-학습-supervised-learning-의-장단점" class="headerlink" title="지도 학습(supervised learning)의 장단점"></a>지도 학습(supervised learning)의 장단점</h2><p>전문가로부터 분류된 training data를 받아 Supervised Learning 방식으로 모델을 학습 시키면 좋은 성능을 얻는 반면, annotation이 표시된 병변으로만 제한이 되는게 단점이다.</p>
<h2 id="비지도-학습-unsupervised-learning-으로-접근한-f-AnoGAn-제안"><a href="#비지도-학습-unsupervised-learning-으로-접근한-f-AnoGAn-제안" class="headerlink" title="비지도 학습(unsupervised learning)으로 접근한 f-AnoGAn 제안"></a>비지도 학습(unsupervised learning)으로 접근한 f-AnoGAn 제안</h2><p>본 논문에서는 biomarker candidates를 할 수 있는 anomalous images 및 image segments를 식별할 수 있는 GAN 기반 Unsupervised Learning 접근법으로 해결하는 f-AnoGAN(fast AnoGAN)을 제안한다.</p>
<h2 id="Fast-mapping-technique-of-new-data"><a href="#Fast-mapping-technique-of-new-data" class="headerlink" title="Fast mapping technique of new data"></a>Fast mapping technique of new data</h2><p>Normal data로 Generator model을 학습 시키고 GAN latent space에서 query data $X$(병변 유무를 확인하고 싶은 테스트 영상)의 fast mapping technique를 제안하고 평가한다. latent space mapping 방법은 Encoder를 기반으로 이루어지며 Discriminator feature residual error 및 image reconstruction error $G(z)$를 포함하는 훈련된 모델을 기반으로 anomaly detection이 진행된다.</p>
<p>Optical Coherence Tomography(OCT) 촬영 데이터를 사용한 딥러닝 학습 관련 실험은 본 논문에서 제안한 방법과 대체 접근법(AnoGAN, BiGAN 등)과 비교하며 실험한다. 본 논문에서 제안한 f-AnoGAN 방법이 anomaly detection의 정확도를 높인다는 포괄적인 경험적 증거를 제공한다. 또한 두 명의 망막 전문가를 대상으로 한 시각적 테스트 결과 생성된 이미지 $G(z)$가 실제 OCT 이미지와 구별이 잘 안되는 것으로 나타났다.</p>
<p><img src="/images/post_images/post0002/f-AnoGAN_figure1.png" alt="그림1"></p>
<center><figcaption> (그림 1) </figcaption></center>


<h1 id="Fast-GAN-based-anomaly-detection"><a href="#Fast-GAN-based-anomaly-detection" class="headerlink" title="Fast GAN-based anomaly detection"></a>Fast GAN-based anomaly detection</h1><p>본 논문에서 제안한 Anomaly Detection 순서는 크게 두가지로 이루어 집니다.</p>
<ul>
<li>normal image만 이용해서 GAN 학습</li>
<li>normal image을 잘 생성할 수 있는 GAN을 기반으로 GAN 학습할 때 사용한 데이터와 동일한 데이터로 Encoder 학습 </li>
</ul>
<p>f-AnoGAN의 핵심은 <code>image를 latent space mapping하는 방법으로 Encoder 모델을 사용한 것</code>이 핵심입니다. 비슷한 방법인 AnoGAN과 비교를 해보면 확실한 차이점을 확인할 수 있습니다.</p>
<h2 id="AnoGAN"><a href="#AnoGAN" class="headerlink" title="AnoGAN"></a>AnoGAN</h2><p><a href="https://mulkong.github.io/2020/10/04/Unsupervised-Anomaly-Detection-With-GAN/">🔗 AnoGAN 정리</a></p>
<p>AnoGAN의 목표는 query image XX가 주어지면 query image $X$와 가장 유사하고 Manifold $X$에 위치하는 $G(z)$의 latent space를 찾는 것 입니다. 이 과정에서 $G(z)$의 Probability density function에 의존하게 됩니다.</p>
<p>mapping된 latent space의 세부적인 과정은 다음과 같습니다.</p>
<ul>
<li>random하게 $z_n$을 뽑아 $G(z_n)$을 얻는다.</li>
<li>query image $X$와 유사한 $G(z)$을 찾기 위해 latent space안에서 최적의 $z$을 찾기 위해 residual loss + discriminator loss을 기반으로 backpropagation을 통해 찾는다. (OCT 기분 500 iteration)</li>
<li>최종적으로 residual loss 수식을 그대로 사용하여 anomaly score을 계산한다.<blockquote>
<p>anomaly score 구하는 방법은 f-AnoGAN에서도 동일하게 적용 됩니다.</p>
</blockquote>
</li>
</ul>
<p>위에서 설명드린 방법은 학습 이미지 크기가 작으면 효율적이고 나름 빠르게 진행이 되지만 이미지 크기가 크면 클수록 그만큼 많은 정보들을 고려해야 하므로 random하게 iteration하며 학습하는 AnoGAN 방법은 mapping이 제대로 안되는 경우가 발생할 수 있습니다.</p>
<h2 id="F-AnoGAN"><a href="#F-AnoGAN" class="headerlink" title="F-AnoGAN"></a>F-AnoGAN</h2><p>f-AnoGAN은 이런 문제를 해결하고자 AutoEncoder에서 아이디어를 얻었다고 할 수 있습니다. AutoEncoder는 입력된 정보들을 잘 설명할 수 있는 latent vector로 차원을 압축 시켰다가 Decoder를 통해 복원하는 간단한 모델입니다. 이 과정에서 사용된 Encoder 모델을 f-AnoGAN에서는 image를 latent space에 mapping하는 모델로 사용된 것 입니다. 이렇게 학습 데이터를 <code>latent space에 mapping 시켜주는 과정은 identity transformation(항등변환)</code> 해주는 과정이라고 할 수 있으며 입력 이미지가 들어가면 그대로 Decoder를 통해 동일한 이미지로 복원하는 identity mapping이라고도 할 수 있습니다.</p>
<h2 id="Latent-space-mapping"><a href="#Latent-space-mapping" class="headerlink" title="Latent space mapping"></a>Latent space mapping</h2><p>GAN 학습은 latent space $Z$ $\rightarrow$ Manifold $X$로 mapping되는 $G(z) = z \rightarrow x$을 생성하지만 Anomaly Detection에서 필요한 $X \rightarrow$ $Z$로 inverse mapping은 불가능 합니다. (이 개념은 AnoGAN 논문에서도 언급된 개념입니다.)</p>
<p>inverse mapping이 안되는 문제점을 해결하고자 AnoGAN에서는 위에서 언급한 방법으로 latent space에 mapping을 해주었지만 f-AnoGAN에서는 Deep Encoder Network를 사용하여 $E(x) = x \rightarrow z$ 즉, inverse mapping이 가능하도록 학습을 진행합니다. 이 방법을 사용하여 f-AnoGAN제안한 방법은 크게 2가지 방법입니다.</p>
<ul>
<li>$z \rightarrow image \rightarrow z$</li>
<li>$image \rightarrow z \rightarrow image$</li>
</ul>
<p>이 두가지 방법 모두 (그림 2)처럼 image에서 latent space ZZ로 mapping 하는 방법입니다.</p>
<p><img src="/images/post_images/post0002/f-AnoGAN_figure2.png"></p>
<center><figcaption> (그림 2) </figcaption></center>

<h3 id="⎮-Training-the-encoder-with-generated-images-ziz-architecture"><a href="#⎮-Training-the-encoder-with-generated-images-ziz-architecture" class="headerlink" title="⎮ Training the encoder with generated images: $ziz$ architecture"></a>⎮ Training the encoder with generated images: $ziz$ architecture</h3><p>학습중 latent space $Z$로 부터 random sampling된 latent vector $z$는 더이상 weight가 update 안되는 fixed weight $G$를 통해 image space에 mapping 되고 Encoder는 이를 다시 latent space $Z$에 mapping 하도록 학습 합니다. 따라서 $ziz$ architecture의 경우 GAN 학습할 때 사용했던 normal image가 필요 없습니다.</p>
<p>$ziz$ architecture로 학습할 때 사용되는 loss는 $z$와 $E(G(z))$의 MSE를 최소화 하는 방법으로 학습이 진행됩니다.</p>
<p>$$ L_{ziz}(z) = \cfrac{1}{d}||z - E(G(z))||^{2} $$</p>
<p>$ziz$ architecture 방법은 학습할 때 생성된 이미지만 잘 mapping될 뿐 query image $X$가 들어오면 제대로 mapping을 못하게 된다는 단점이 있어 f-AnoGAN 논문에서는 해당 방법으로 학습을 진행 안합니다..</p>
<h3 id="⎮-Training-the-encoder-with-real-images-izi-architecture"><a href="#⎮-Training-the-encoder-with-real-images-izi-architecture" class="headerlink" title="⎮ Training the encoder with real images: $izi$ architecture"></a>⎮ Training the encoder with real images: $izi$ architecture</h3><p>학습하는 동안 real image에서 latent space $Z$로 encoding되어 mapping하는 과정은 Encoder model로 이루어 집니다. 이런 과정을 통해서 query image $X$가 들어오면 AnoGAN에서 사용한 iteration을 시켜주는 방법 없이 빠르게 $X$와 구조적으로 일치한 $G(E(x))$를 생성할 수 있게 됩니다.</p>
<p>$izi$ architecture로 학습할 때 사용되는 Losss는 $x$와 $E(G(x))$의 MSE를 최소화 하는 방법으로 학습이 진행됩니다.</p>
<p>$$ L_{izi}(x) = \cfrac{1}{n}||x - G(E(x))||^{2} $$</p>
<p>$izi$ architecture는 GAN을 학습할 때 사용한 학습 데이터를 이용해서 학습이 진행 됩니다.</p>
<h4 id="izi-architecture의-단점"><a href="#izi-architecture의-단점" class="headerlink" title="$izi$ architecture의 단점"></a>$izi$ architecture의 단점</h4><p>$X$가 latent space $Z$에서 정확하게 어디에 위치하는지 $ziz$ architecture는 $G(z)$를 바로 latent space에 mapping 시켜서 알 수 있었지만 $izi$ architecture는 알 수 없습니다. 따라서 image space로 다시 mapping 하고 image−to−image residual loss를 계산하여 image−to−$z$ mapping의 정확도를 간접적으로 측정할 수 밖에 없습니다.</p>
<p>이런 방법으로 진행하면 query image로 normal image가 들어오면 괜찮지만 abnormal image가 들어오면 residual 이 적은 이미지가 생성될 수 있습니다. 따라서 이런 문제점으로 본 논문에서는 feature space를 loss로 사용해서 mapping이 이루어질 수 있도록 $izif$ architecture를 고안하게 되었습니다.</p>
<h3 id="⎮-Discriminator-guided-izi-encoder-training-izif-architecture"><a href="#⎮-Discriminator-guided-izi-encoder-training-izif-architecture" class="headerlink" title="⎮ Discriminator guided $izi$ encoder training: $izif$ architecture"></a>⎮ Discriminator guided $izi$ encoder training: $izif$ architecture</h3><p>$izi$ architecture 문제점을 해결하기 위해 Discriminator model의 중간 계층인 feature space에서 residual loss를 추가적으로 사용하여 Encoder model을 학습합니다</p>
<p>$$ L_{izif}(x) = \cfrac{1}{n}\cdot||x - G(E(X))||^{2} + \cfrac{k}{n_d}\cdot||f(x) - f(G(E(x)))||^{2}  $$</p>
<blockquote>
<p>nd: 중간 계층의 feature space의 dimension<br>k: weight factor</p>
</blockquote>
<p>수식을 보면 Discriminator feature space는 AnoGAN에서 사용된 loss와 관련이 있는것을 볼 수 있습니다. Discriminator feature는 GAN을 학습할 때 얻은 parameter들을 사용한 것 입니다. feature space를 loss로 사용하게 되면 image space와 latent space에서 잘 mapping이 되도록 약간의 guide를 제공해주는 역할을 하게 됩니다.</p>
<h2 id="Detection-of-anomalies"><a href="#Detection-of-anomalies" class="headerlink" title="Detection of anomalies"></a>Detection of anomalies</h2><p>GAN도 학습이 잘 되었고 Encoder도 학습이 잘 되었다면 이제 query image $X$를 입력으로 넣어서 anomaly score를 계산하면 됩니다. anomaly score를 계산하는 것은 image level의 anomaly detection중 query image와 reconstruction image의 deviation을 score로 나타내야 합니다.</p>
<p>이 score를 anomaly score라고 하며 수식은 아래와 같습니다.</p>
<p><img src="/images/post_images/post0002/f-AnoGAN_figure3.png" alt="그림1"></p>
<center><figcaption> (그림 3) </figcaption></center>

<p>수식을 보면 $loss_{izif}$ 수식과 동일한 것을 볼 수 있습니다.</p>
<p>일반적으로 수식 모두 abnormal image에서 높은 anomaly score, normal image는 낮은 anomaly score가 계산됩니다. 모델은 normal image에 대해서만 학습하므로 입력 이미지와 시각적으로 유사한 이미지만 normal image의 Manifold $X$에 눞혀서 Encoder를 통해 재구성 됩니다.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="ko">
    <link itemprop="mainEntityOfPage" href="http://mulkong.github.io/2020/10/04/Unsupervised-Anomaly-Detection-With-GAN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="mulkong">
      <meta itemprop="description" content="공부한 내용을 바탕으로 정리한 기술블로그 입니다.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mulkong DeepLearning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/04/Unsupervised-Anomaly-Detection-With-GAN/" class="post-title-link" itemprop="url">[AnoGAN]Unsupervised Anomaly Detection with GAN</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">작성일</span>

      <time title="Post created: 2020-10-04 19:01:01" itemprop="dateCreated datePublished" datetime="2020-10-04T19:01:01+09:00">2020-10-04</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Updated at: 2021-05-05 14:59:10" itemprop="dateModified" datetime="2021-05-05T14:59:10+09:00">2021-05-05</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/GAN/" itemprop="url" rel="index"><span itemprop="name">GAN</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <hr>
<ul>
<li>이번 포스팅은 티스토리에서 깃블로그로 이사 후 티스토리에 정리했던 내용에서 내용을 추가해서 작성한 글 입니다. 이전 글은 <a target="_blank" rel="noopener" href="https://sensibilityit.tistory.com/506?category=731657">AnoGAN 정리글_Tistory</a>에서 보실 수 있습니다.</li>
<li>기존 Anomaly Detection은 Supervised Learning으로 접근했지만 AnoGAN은 GAN을 이용한 Unsupervised Learning 방법으로 접근하여 Anomaly Detection하는 논문입니다.</li>
<li>Paper 원문: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1703.05921">Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery</a></li>
</ul>
<hr>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><h2 id="GAN이란"><a href="#GAN이란" class="headerlink" title="GAN이란?"></a>GAN이란?</h2><p>GAN 학습 과정은 Discriminator가 Real/Fake를 잘 맞추도록 학습한 후 Generator가 생성한 Fake image가 Discriminator를 속여 Real이라고 말하게끔 하도록 하는 학습 과정입니다.</p>
<h2 id="DCGAN"><a href="#DCGAN" class="headerlink" title="DCGAN"></a>DCGAN</h2><h3 id="⎮-CNN-vs-MLP"><a href="#⎮-CNN-vs-MLP" class="headerlink" title="⎮ CNN vs MLP"></a>⎮ CNN vs MLP</h3><ul>
<li><p>MLP(Multi-Layer Perceptron)</p>
<ul>
<li>특징: 3차원 데이터를 1개의 Vector로 풀어서 인식합니다. </li>
<li>단점: 이미지 위치 정보를 무시하게 됩니다. (Feature loss 발생)</li>
</ul>
</li>
<li><p>CNN(Convolutional Nerual Network)</p>
<ul>
<li>특징: 3차원 데이터 입력을 그대로 사용해 위치 정보가 반영됩니다.</li>
</ul>
</li>
</ul>
<h3 id="⎮-Discriminator"><a href="#⎮-Discriminator" class="headerlink" title="⎮ Discriminator"></a>⎮ Discriminator</h3><p>Convolution 연산을 통해 데이터의 차원을 줄이는 과정입니다. 어떻게 보면 Encoder의 모델 구조와 비슷하다고 볼 수 있습니다.</p>
<h3 id="⎮-Generator"><a href="#⎮-Generator" class="headerlink" title="⎮ Generator"></a>⎮ Generator</h3><p>Generator는 Latent Space를 입력으로 넣어주면 차원을 확장 시키는 방법으로 모델 구조를 구성합니다. 이때 사용하는 함수는 Pytorch 기준 nn.ConvTranspose2d를 사용합니다. 즉, Deconvolution 방식으로 모델이 구성됩니다.</p>
<h3 id="⎮-DCGAN-특징"><a href="#⎮-DCGAN-특징" class="headerlink" title="⎮ DCGAN 특징"></a>⎮ DCGAN 특징</h3><ol>
<li><p><strong>Generator는 이미지를 외워서 보여주는 것이 아니다. (Memorization이 안일어난다.)</strong><br> Paper를 읽어보면 “walking in the latent space”라는 문구가 나옵니다. 만약 G가 Memorization이 발생된다면 latent vector를 조금씩 바꿀 때 이미지가 변형되 못할 것 입니다.</p>
</li>
<li><p><strong>Memorization이 일어난 경우는 어떤 경우인가?</strong><br> Generator가 유의미한 특징들을 학습하지 않고 overfitting이 발생되 데이터와 1:1 mapping이 발생하는 Identity Mapping 학습이 이루어지게 됩니다.</p>
</li>
<li><p><strong>walking in the latent space</strong><br> latept space 내부에서 vector값을 조금씩 변경하면 부드럽게 이미지들이 변화되는것을 확인할 수 있는 그림 입니다.</p>
</li>
</ol>
<p><img src="/images/post_images/walking_latent_space_DCGAN.jpg"></p>
<h1 id="AnoGAN"><a href="#AnoGAN" class="headerlink" title="AnoGAN"></a>AnoGAN</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>의료데이터에서 disease progression과 treatment monitoring에 이미지를 검출하는 model을 구하는 것은 어려운 일이다. 기존 모델은 automating detection을 위해 annotation이 이루어진 많은 양의 데이터를 기반으로 Supervised Learning 방식으로 진행했다. Supervised Learning은 annotation과 이미 알려진 것들에 대해서만 detection이 이루어 지는 단점이 존재하다. 그래서 이 논문은 Unsupervised Learning 방식으로 학습을 진행한 후 Anomaly Detection 과정이 이루어진다. 이 논문에서 제안한 <strong>AnoGAN</strong>은 다양한 Normal Data로 학습한 DCGAN과 Image space 안에서 latent space의 mapping을 기반으로 Anomaly score를 계산한다. 새로운 데이터가 들어오게 되면 model은 anomaly한 부분에 labeling하게 되고 Normal image로 학습된 distribution에 적합한지 image patches에 anomaly score를 나타낸다. 이 방식은 망막의 광학 단층 촬영 영상에 적용한 결과 망막 유체 또는 반사성 초점을 포함하는 이미지와 같은 Abnormal image을 정확하게 식별함을 확인했다.</p>
<h2 id="AnoGAN이-등장한-계기"><a href="#AnoGAN이-등장한-계기" class="headerlink" title="AnoGAN이 등장한 계기"></a>AnoGAN이 등장한 계기</h2><p>이 논문에서 사용된 데이터는 의료 데이터 입니다. 의료현장, 산업현장 모두 공통적으로 Normal 데이터는 많지만 Abnormal 데이터는 부족한 경우가 많습니다. 지금까지 Anomaly Detection을 하기 위해 Supervised Learning 방법으로 진행했지만 이는 Anotation된 데이터가 많이 필요하다는 단점이 발생되게 됩니다. 그래서 Unsupervised Learning 방법 중 GAN을 이용한 Anomaly Detection을 진행하게 되었습니다.</p>
<p>좀 더 쉽게 정리를 해보면…<br>Discriminator는 입력 이미지가 True/False의 확률을 구하는 classifier라고 생각하시면 됩니다. 여기서 Unsupervised Learning으로 접근하는건 굳이 Annotation 과정을 할 필요 없이 모델 알아서 처리를 하는 과정으로 접근한다는 것 입니다. Discriminator을 학습할 때 그 discribution 안에 속하지 않은 데이터들은 다 Fake라고 하지 안을까? 라는 아이디어에서 Normal Discribution의 outlier detection을 할 수 있는 AnoGAN이 탄생하게 된 것 입니다.</p>
<h2 id="Unsupervised-Learning으로-접근한-이유"><a href="#Unsupervised-Learning으로-접근한-이유" class="headerlink" title="Unsupervised Learning으로 접근한 이유"></a>Unsupervised Learning으로 접근한 이유</h2><h3 id="⎮-Data-Imbalance-Problem"><a href="#⎮-Data-Imbalance-Problem" class="headerlink" title="⎮ Data Imbalance Problem"></a>⎮ Data Imbalance Problem</h3><p>위에서도 잠깐 언급을 하긴 했지만 의료데이터, 산업현장 데이터는 Normal 데이터는 많지만 Abnormal 데이터는 부족한 점이 특징입니다. 그래서 Supervised Learning으로 문제를 해결하려면 우선적으로 Data Imbalance 문제를 해결해야합니다. 이런 수고스러움을 덜고자 Unsupervised Learning 방법으로 접근하게 되었습니다.</p>
<h3 id="⎮-Annotation-작업의-엄청난-비용"><a href="#⎮-Annotation-작업의-엄청난-비용" class="headerlink" title="⎮ Annotation 작업의 엄청난 비용"></a>⎮ Annotation 작업의 엄청난 비용</h3><p>아무래도 의료데이터는 비전문가들이 annotation을 할 수 없다는 제한점이 있다보니 전문가들이 직접 annotation을 해야합니다. 해보신 분들은 하시겠지만 엄청난 시간과 비용을 투자해야한다는 단점이 발생합니다. 또한 균일하게 완벽하게 100%로 annotation을 한다는 보장이 없다보니 데이터 마다 성능차이가 많이 발생할 수 있다는 문제가 존재합니다. 그래서 이런 과정을 하지 않고 Normal 데이터로만 학습 한 후 Normal Discribution에서 벗어나는 데이터가 들어올때 어느 부분이 outlier인지 detection 해주는 방법으로 진행했습니다.</p>
<h2 id="AnoGAN의-동작-원리"><a href="#AnoGAN의-동작-원리" class="headerlink" title="AnoGAN의 동작 원리"></a>AnoGAN의 동작 원리</h2><h3 id="⎮-정상-데이터로-GAN-학습"><a href="#⎮-정상-데이터로-GAN-학습" class="headerlink" title="⎮ 정상 데이터로 GAN 학습"></a>⎮ 정상 데이터로 GAN 학습</h3><img src="/images/post_images/AnoGAN_step1.png" width="70%" height="70%">
Generator Model이 어떤 latent space가 들어와도 Normal 이미지를 잘 생성할 수 있도록 GAN을 Normal 데이터로만 학습시켜야 합니다. 그러면 Generator Model은 Normal Manifold를 학습하게 되어서 Normal 데이터만 생성하게 됩니다.

<h3 id="⎮-최적의-Z값-찾기"><a href="#⎮-최적의-Z값-찾기" class="headerlink" title="⎮ 최적의 Z값 찾기"></a>⎮ 최적의 Z값 찾기</h3><img src="/images/post_images/AnoGAN_step2.png" width="70%" height="70%">
Generator와 Discriminator의 Parameter를 Fix시켜 더이상 Update 해주지 않는 상태로 latent vector $z_1$을 random sampling 시킨 후 Generator에 입력 데이터로 넣어준다 $G(z_1)$. Generator는 Normal 데이터의 Manifold를 학습한 상태여서 Normal image를 생성하게 된다.


<h3 id="⎮-Residual-Loss"><a href="#⎮-Residual-Loss" class="headerlink" title="⎮ Residual Loss"></a>⎮ Residual Loss</h3><img src="/images/post_images/AnoGAN_step3.png" width="70%" height="70%">
query image와 $G(z_1)$ 사이에서 다른 부분이 있는지 그 차이를 비교하는 과정입니다.

<h3 id="⎮-Discrimination-Loss"><a href="#⎮-Discrimination-Loss" class="headerlink" title="⎮ Discrimination Loss"></a>⎮ Discrimination Loss</h3><img src="/images/post_images/AnoGAN_step4.png" width="70%" height="70%">
Discriminator의 역할은 True/False를 판별해주는 역할을 갖고 있습니다. 이런 기능을 이용해서 들어오는 입력 데이터들의 확률분포(Probability Distribution)을 파악해서 True/False를 판단해줄 수 있습니다. 즉, Discrimination Loss는 $G(z_r)$의 Manifold 또는 Data Distribution에 잘 Mapping되도록 패널티를 부과하는 Loss 입니다. 다만, 이때 Discriminator의 중간 레이어에서 뽑은 Feture를 이용해서 계산을 하고 있습니다. 그 이유는 논문에서 중간층이 더 많은 표현력을 갖고 있다고 나와있습니다. 이러한 과정을 Feature Mapping이라고 불립니다.


<p>$$<br>L_D(z_r) = \sum |f(x) - f(G(z_r))|<br>$$</p>
<h3 id="⎮-Residual-Discrimination-Loss"><a href="#⎮-Residual-Discrimination-Loss" class="headerlink" title="⎮ Residual + Discrimination Loss"></a>⎮ Residual + Discrimination Loss</h3><img src="/images/post_images/AnoGAN_step5.png" width="70%" height="70%">


<p>$$<br>L(z_r) = (1 - \lambda) \cdot L_R(z_r) + \lambda \cdot L_D(z_r)<br>$$</p>
<p>$z_1$에 대한 L(z_r)를 구하는 과정입니다. 이 과정은 Generatir와 Discriminator의 weight를 Fixed 시켜주고 $L(z_r)$가 최소가 되도록 latent vector를 Gradient Descent 과정을 통해서 조정 시켜줍니다. </p>
<p>따라서 $z_1 \rightarrow z_2 \rightarrow z_3 \rightarrow …. z_r$로 여러번 iteration 시켜주면서 제대로 query image와 $G(z)$와 mapping이 제대로 이루어지는 값을 찾는 과정을 진행합니다. (OCT 데이터셋은 500번 iteration 해주었습니다)</p>
<p>또한 $L(z_r)$를 Anomaly Score로 사용해서 판단의 기준으로 사용하게 됩니다.</p>
<h1 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h1><h2 id="DCGAN-1"><a href="#DCGAN-1" class="headerlink" title="DCGAN"></a>DCGAN</h2><p>DCGAN 원 논문에서 사용한 데이터는 RGB(3channels)이였습니다. 그렇지만 OCT 영상 데이터는 gray-scale(1channel)이여서 parameter만 변경을 시켜준 상태로 사용을 했습니다.<br><img src="/images/post_images/DCGAN.png" width="70%" height="70%"></p>
<h2 id="AnoGAN-1"><a href="#AnoGAN-1" class="headerlink" title="AnoGAN"></a>AnoGAN</h2><img src="/images/post_images/AnoGAN.png" width="70%" height="70%">

<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>AnoGAN 전 과정을 요약 해보자면</p>
<ol>
<li>Normal Data로만 GAN을 학습 시킵니다. 잘 학습된 GAN은 Normal 이미지만 생성하게 됩니다.</li>
<li>latent space에서 $z_r$값을 random sampling 해줍니다.</li>
<li>Sampling해준 $z_r$을 이용해 query data와 비슷하게 생성되는지 $G(z_r)$와 query image와 비교를 합니다. 이 과정은 inference 과정에서 이루어짐니다.</li>
<li>query image와 $G(z_r)$이 비슷한 Distribution으로 mapping이 되었다면 비정상 query image가 들어왓다고 해도 $G(z_r)$은 정상이지만 query image와 구조적으론 똑같은 이미지로 생성이 될 것 입니다. 이 때 Anomaly Score를 계산해줍니다.</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>





  



      

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

    </div>
  </main>

  <footer class="footer">
    <div class="footer-inner">
      

      

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">mulkong</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.0/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  


















  








  

  
      <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = '//cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
