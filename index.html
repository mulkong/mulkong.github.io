<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.0/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"mulkong.github.io","root":"/","scheme":"Muse","version":"8.0.1","exturl":false,"sidebar":{"position":"right","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}};
  </script>

  <meta name="description" content="공부한 내용을 바탕으로 정리한 기술블로그 입니다.">
<meta property="og:type" content="website">
<meta property="og:title" content="Mulkong DeepLearning">
<meta property="og:url" content="http://mulkong.github.io/index.html">
<meta property="og:site_name" content="Mulkong DeepLearning">
<meta property="og:description" content="공부한 내용을 바탕으로 정리한 기술블로그 입니다.">
<meta property="og:locale" content="ko_KR">
<meta property="article:author" content="mulkong">
<meta property="article:tag" content="DeepLearning, AI, Pytorch, Python, MachineLearning, Programming">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://mulkong.github.io/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'ko'
  };
</script>

  <title>Mulkong DeepLearning</title>
  






  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Mulkong DeepLearning</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">딥러닝 정리 블로그</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>홈</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>태그</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>카테고리</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>아카이브</a>

  </li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          목차
        </li>
        <li class="sidebar-nav-overview">
          흝어보기
        </li>
      </ul>

      <!--noindex-->
      <section class="post-toc-wrap sidebar-panel">
      </section>
      <!--/noindex-->

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">mulkong</p>
  <div class="site-description" itemprop="description">공부한 내용을 바탕으로 정리한 기술블로그 입니다.</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">4</span>
          <span class="site-state-item-name">포스트</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">카테고리</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">태그</span>
      </div>
  </nav>
</div>



      </section>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">
      

      
    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="ko">
    <link itemprop="mainEntityOfPage" href="http://mulkong.github.io/2021/12/25/GAN%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%9C-%ED%9A%A8%EC%9C%A8%EC%A0%81%EC%9D%B8-Anomaly-Detection-%EB%B0%A9%EB%B2%95-f-AnoGAN-vs-MemAE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="mulkong">
      <meta itemprop="description" content="공부한 내용을 바탕으로 정리한 기술블로그 입니다.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mulkong DeepLearning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/25/GAN%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%9C-%ED%9A%A8%EC%9C%A8%EC%A0%81%EC%9D%B8-Anomaly-Detection-%EB%B0%A9%EB%B2%95-f-AnoGAN-vs-MemAE/" class="post-title-link" itemprop="url">GAN을 이용한 효율적인 Anomaly Detection 방법 [f-AnoGAN vs MemAE]</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">작성일</span>

      <time title="Post created: 2021-12-25 13:18:02" itemprop="dateCreated datePublished" datetime="2021-12-25T13:18:02+09:00">2021-12-25</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Updated at: 2021-12-26 19:06:12" itemprop="dateModified" datetime="2021-12-26T19:06:12+09:00">2021-12-26</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/GAN/" itemprop="url" rel="index"><span itemprop="name">GAN</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <hr>
<ul>
<li>이번 포스팅은 티스토리에서 깃블로그로 이사 후 티스토리에 정리했던 내용에서 내용을 추가해서 작성한 글 입니다. 이전 글은 <a target="_blank" rel="noopener" href="https://sensibilityit.tistory.com/518">Tistory</a>에서 보실 수 있습니다.</li>
</ul>
<hr>
<p>Unsupervised Learning 방법으로 GAN을 이용한 Anomaly Detection 방법 중 Encoder 부분을 이용한 f-AnoGAN 방법이 있습니다. 이 방법은 효율적으로 Anomaly Detection을 잘 하지만 미세한 결함을 제대로 검출하기 힘들다는 한계점이 있습니다. 본 글에서는 f-AnoGAN의 특징들과 단점에 대해 간략히 소개를 하고 그 해결책에 대한 내용을 간략하게 정리해보기 위한 글 입니다.</p>
<p>비교를 할 논문은 아래와 같습니다.</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/abs/pii/S1361841518302640">f-AnoGAN:Fast unsupervised anomaly detection with generative adversarial networks</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.02639">MemAE:Memorizing Normality to Detect Anomaly: Memory-augmented Deep Autoencoder for Unsupervised Anomaly Detection</a></li>
</ul>
<hr>
<h1 id="f-AnoGAN"><a href="#f-AnoGAN" class="headerlink" title="f-AnoGAN"></a>f-AnoGAN</h1><p><strong>참고 링크</strong></p>
<ul>
<li>blog.promedius.ai/f-anogan-fast-unsupervised-anomaly-detection-with-gan/</li>
</ul>
<p><strong>Github tutorial</strong></p>
<ul>
<li>github.com/mulkong/f-AnoGAN_with_Pytorch</li>
</ul>
<p>f-AnoGAN의 학습 방법은 GAN 학습, Encoder 학습으로 총 2가지 step으로 이루어집니다.</p>
<img src="/images/post_images/post0004/figure_0001.png" width="100%" height="100%">

<hr>
<h2 id="GAN-학습"><a href="#GAN-학습" class="headerlink" title="GAN 학습"></a>GAN 학습</h2><p>정상 데이터로만 GAN 학습을 진행합니다. GAN이 잘 학습이 되었다면 정상 데이터에 대한 학습 분포를 기반으로 학습이 이루어져 정상 이미지만을 생성할 가능성이 높아집니다.</p>
<h2 id="Encoder-학습"><a href="#Encoder-학습" class="headerlink" title="Encoder 학습"></a>Encoder 학습</h2><p>GAN이 잘 학습 된 상태라면 이제 입력된 query data에 대해 Latent space mapping을 위한 Encoder 모델 학습을 진행합니다.</p>
<p>이와 같은 방식으로 진행한 이유는 query data(정상/비정상) 중 정상 데이터가 입력되면  Encoder을 통해서 정상 query data에 대해 latent space mapping된 Z값을 이용하여 Generator에 Z가 입력되게 됩니다. 그러면 Generator는 query data에 대한 정상 데이터를 생성하게 됩니다.</p>
<p>반면 비정상 query data가 입력으로 주어지면 Encoder는 query data에 대한 Feature을 추출해서 Z값으로 만든 후 비정상 query data의 Feature을 Generator에 입력으로 주어지면 query data을 기반으로 한 이미지가 생성됩니다.</p>
<p>GAN은 정상 이미지로만 학습이 진행되어 정상 이미지만을 생성하려고 할 것 입니다. 따라서 비정상 query data의 모양, 질감, 형태와 동일하지만 정상 상태로 이미지를 생성하게 되어 이상 탐지(anomaly detecTion)을 수행합니다.</p>
<h3 id="단점"><a href="#단점" class="headerlink" title="단점"></a>단점</h3><p>일반적으로 AutoEncoder을 이용한 Anomaly Detection을 진행하다보면 AutoEncoder의 특성상 일반화(Generalization)을 너무 잘 해서 정상만 생성하도록 해야하는데 비정상까지 재구성하게 되어 미세한 결함을 제대로 찾기 힘들다는 단점이 존재합니다.</p>
<h3 id="해결책"><a href="#해결책" class="headerlink" title="해결책"></a>해결책</h3><p>memory module을 사용하여 AutoEncoder을 augment(보강)하는 MemAE(memory-augmented autoencoder) 방법을 사용.</p>
<hr>
<h1 id="MemAE"><a href="#MemAE" class="headerlink" title="MemAE"></a>MemAE</h1><p>MemAE의 핵심 내용은 AutoEncoder가 너무 general하게 학습 되는 경우가 발생하여 정상 뿐만 아니라 간혹 비정상의 결함 부분 까지 포함하여 생성하게 된다는 단점이 존재하니 이를 해결하기 위해 정상 데이터을 Encoding할 때 정상 데이터에 대한 memory을 얻은 후 이를 기반으로 해서 정상 데이터를 생성하는 방법입니다.</p>
<img src="/images/post_images/post0004/figure_0002.png" width="100%" height="100%">

<p>학습 단계에서 memory는 정상 데이터에 가장 관련성이 높은 메모리 항목을 검색하며 데이터를 생성(재구성)하게 됩니다. 테스트 과정시 정상 데이터에 가장 관련성이 높은 메모리를 고정시킨 후 query data가 입력으로 주어지면 query data을 기반으로 한 정상 데이터를 메모리 기록으로 부터 검색을한 후 이를 기반으로 생성(재구성)이 이루어지게 됩니다.</p>
<hr>
<p>다음 포스팅할 글은 MemAE 논문에 대해 정리한 글을 작성하도록 하겠습니다.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="ko">
    <link itemprop="mainEntityOfPage" href="http://mulkong.github.io/2021/12/24/Improved%20anomaly%20detection%20by%20training%20an%20autoencoder%20with%20skip%20connections%20on%20images%20corrupted%20with%20stain%20shaped%20noise/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="mulkong">
      <meta itemprop="description" content="공부한 내용을 바탕으로 정리한 기술블로그 입니다.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mulkong DeepLearning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/12/24/Improved%20anomaly%20detection%20by%20training%20an%20autoencoder%20with%20skip%20connections%20on%20images%20corrupted%20with%20stain%20shaped%20noise/" class="post-title-link" itemprop="url">[Anomaly Detection] Improved anomaly detection by training an autoencoder with skip connections on images corrupted with Stain-shaped noise</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">작성일</span>
      

      <time title="Post created: 2021-12-24 21:44:10 / Updated at: 21:45:32" itemprop="dateCreated datePublished" datetime="2021-12-24T21:44:10+09:00">2021-12-24</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/GAN/" itemprop="url" rel="index"><span itemprop="name">GAN</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <hr>
<ul>
<li>이번 포스팅은 티스토리에서 깃블로그로 이사 후 티스토리에 정리했던 내용에서 내용을 추가해서 작성한 글 입니다. 이전 글은 <a target="_blank" rel="noopener" href="https://sensibilityit.tistory.com/517">Tistory</a>에서 보실 수 있습니다.</li>
<li>Paper 원문: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2008.12977">Improved anomaly detection by training an autoencoder with skip connections on images corrupted with Stain-shaped noise</a></li>
</ul>
<hr>
<h1 id="1-Abstract"><a href="#1-Abstract" class="headerlink" title="1.Abstract"></a>1.Abstract</h1><h3 id="산업-현장에서-AE를-활용한-Anomaly-Detection"><a href="#산업-현장에서-AE를-활용한-Anomaly-Detection" class="headerlink" title="산업 현장에서 AE를 활용한 Anomaly Detection"></a>산업 현장에서 AE를 활용한 Anomaly Detection</h3><p>Industrial vision에서 Anomaly Detection problems은 결함이 있거나 없는 arbitrary image를 clean image에서 mapping하도록 훈련된 AutoEncoder를 사용하여 해결할 수 있습니다.</p>
<h3 id="skip-connections이-있는-AutoEncoder-AES-를-사용한-이유"><a href="#skip-connections이-있는-AutoEncoder-AES-를-사용한-이유" class="headerlink" title="skip-connections이 있는 AutoEncoder(AES)를 사용한 이유"></a>skip-connections이 있는 AutoEncoder(AES)를 사용한 이유</h3><p>이 접근 방식에서 Anomaly Detection과정은 개념적으로 본다면 reconstruction residual 또는 reconstruction uncertainty에 의존합니다. 공통적으로 sharpness of the reconstruction를 높이기 위해 skip-connections이 있는 AutoEncoder를 고려하게 되었습니다.</p>
<h3 id="AES-Stain-noise-model-제안"><a href="#AES-Stain-noise-model-제안" class="headerlink" title="AES + Stain noise model 제안"></a>AES + Stain noise model 제안</h3><p>Reconstruction 과정 중 clean image가 나오게끔 하기 위해서 train image의 Clean image만 학습으로 진행하는 전략은, reconstruction을 담당하는 Network가 입력하는 대로 출력을 해버리는 identity mapping으로 수렴 되는것을 방지하기 위해 Noise model로 train image를 손상시키고 출력으로 Clean image가 나오도록 하는 목적으로 Stain noise model을 추가 할 것을 제안합니다.</p>
<h3 id="우리가-사용한-모델-짱짱👍-→-AES-Stain-model"><a href="#우리가-사용한-모델-짱짱👍-→-AES-Stain-model" class="headerlink" title="우리가 사용한 모델 짱짱👍 → AES+Stain model"></a>우리가 사용한 모델 짱짱👍 → AES+Stain model</h3><p>이 모델을 실제 결함 모양에 관계없이 임의의 실제 이미지에서 깨끗한 이미지를 재구성 하는 데 유리하다는 것을 보여줍니다.</p>
<h3 id="우리가-가라로-안했다는것을-증명하기-위해-이런-데이터셋으로-진행했다"><a href="#우리가-가라로-안했다는것을-증명하기-위해-이런-데이터셋으로-진행했다" class="headerlink" title="우리가 가라로 안했다는것을 증명하기 위해 이런 데이터셋으로 진행했다"></a>우리가 가라로 안했다는것을 증명하기 위해 이런 데이터셋으로 진행했다</h3><p>우리의 접근 방식의 관련성을 입증하는 것 외에도 우리의 검증은 pixel-wise및 image-wise anomaly detection을 위해 MVTec AD dataset에 대한 성능을 비교하여 reconstruction-based 방법에 대한 일관된 평가를 제공합니다.</p>
<hr>
<h1 id="2-Methods"><a href="#2-Methods" class="headerlink" title="2.Methods"></a>2.Methods</h1><h3 id="Anomaly-Detection-개요"><a href="#Anomaly-Detection-개요" class="headerlink" title="Anomaly Detection 개요"></a>Anomaly Detection 개요</h3><p>Anomaly detection은 Clean(Normal) data의 Distribution에 속하지 않는 다른 데이터들을 식별하는 작업으로 정의할 수 있습니다.</p>
<h3 id="Supervised-Learning으로-하는-Anomaly-Detection의-문제점"><a href="#Supervised-Learning으로-하는-Anomaly-Detection의-문제점" class="headerlink" title="Supervised Learning으로 하는 Anomaly Detection의 문제점"></a>Supervised Learning으로 하는 Anomaly Detection의 문제점</h3><p>Clean(Normal) data에 비해 Defective(Abnormal) data를 수집하는 것은 한계가 있고 만들다 하더라도 Data Imbalance가 발생합니다.</p>
<p>본 논문은 Unsupervised Learning으로 Anomaly Detection을 해결하는 논문입니다. 다만 이전에 정리했었던 AnoGAN, f-AnoGAN와의 차이점은 Query image가 입력으로 들어오면 그것을 구조는 동일하지만 Normal image로 image generation해주는 방법으로 GAN을 사용했습니다. 그러나 이 논문은 GAN이 mode collapse의 문제점을 지적하고 image generation 해주는 대신 image reconstruction 해주는 AutoEncoder 구조를 사용했습니다.</p>
<p>🤔 <strong><em>제 생각이지만 해당 저자는 f-AnoGAN과 AnoGAN의 방법만 인용했지, GAN 모델을 수정한 노력은 없어 보였습니다. 그래서 High resolution을 잘 생성해주는 GAN 모델들을 이용하면 mode collapse 문제는 많이 없어질텐데…좀 아쉽네요</em></strong></p>
<hr>
<h1 id="3-Training-Step"><a href="#3-Training-Step" class="headerlink" title="3.Training Step"></a>3.Training Step</h1><img src="/images/post_images/post0003/figure_0001.png" width="100%" height="100%">

<p>위 이미지는 Unsupervised Learning 방법으로 Anomaly Detection 하는 전체 Architecture framework를 보여주고 있습니다.</p>
<h3 id="Blue-box"><a href="#Blue-box" class="headerlink" title="Blue box"></a>Blue box</h3><p>위 모델의 구조를 살펴보고 해석 해보도록 하겠습니다. 일단 Blue box를 보시면 GAN 같은 경우 Normal image를 generation 해주는 용도로 사용이 되지만, 여기서는 <code>Skip-connections를 사용한 AutoEncoder(AES) 모델을 사용해서 입력 이미지로 query image가 들어오면 그것을 어떤 이미지가 들어오든 Normal image로 reconstruction 해주는 방법</code>입니다. </p>
<p>Stain Model을 사용한 이유는 AutoEncoder는 입력 이미지가 들어오면 그것을 그대로 복원해주는 성질이 있습니다. 근데 Denoising AutoEncoder를 생각해보면 noise가 있는 입력 이미지가 들어오면 그것을 선명한 이미지로 reconstruction해주는 성질이 있어서 일반 AutoEncoder를 사용할때 보다 더 선명하게 reconstruction 해주는 경향이 있습니다. </p>
<p>마찬가지로 해당 논문에서는 얼룩 무늬를 추가해주는 Stain noise model을 이용해서 입력 이미지를 결함이 있는 이미지로 만들어줘서 다시 깨끗한 이미지로 복원될 수 있도록 하는 역할을 가지고 있습니다. 즉, Identity mapping이 안되도록 방지해주기 위해 Stain noise model을 사용하는 것 입니다.</p>
<p>좀 더 자세히 모델을 살펴보도록 하겠습니다.</p>
<img src="/images/post_images/post0003/figure_0002.png" width="100%" height="100%">

<p>이 논문에서 사용한 데이터 크기는 256x256인 오직 Clean(Normal) Image로 학습되었습니다. 여기서 위 이미지(AE skip-connection architecture)처럼 <code>bottleneck 구조를 갖는 Skip-connection 을 사용함으로 써 model에 projection한 임의의 이미지로부터 좀 더 선명한 이미지가 reconstruction</code> 된다고 합니다.</p>
<p>AESc(AE skip-connection) 모델 구조를 보면 U-Net과 모양이 비슷하다는 생각이 자연스럽게 들게 됩니다. 그래서 한번 U-Net과 AESc를 비교 해보았습니다.</p>
<h2 id="3-1-AESc-vs-U-Net"><a href="#3-1-AESc-vs-U-Net" class="headerlink" title="3-1. AESc vs U-Net"></a>3-1. AESc vs U-Net</h2><img src="/images/post_images/post0003/figure_0003.png" width="100%" height="100%">

<h3 id="Autoencoder"><a href="#Autoencoder" class="headerlink" title="Autoencoder"></a>Autoencoder</h3><p>Autoencoder는 <code>입력 이미지에서 핵심 Feature들만 뽑는 구조</code>이며 Bottleneck을 갖고 있습니다. Bottleneck 구조를 갖는 모델은 Image-to-Image translation tasks에서 많이 사용되는 구조이며 <code>결과 이미지의 형태들이 급진적으로 변화하는 특징</code>을 갖고있습니다.</p>
<h5 id="Bottleneck-구조를-갖는-AutoEncoder-구조의-장단점"><a href="#Bottleneck-구조를-갖는-AutoEncoder-구조의-장단점" class="headerlink" title="Bottleneck 구조를 갖는 AutoEncoder 구조의 장단점"></a>Bottleneck 구조를 갖는 AutoEncoder 구조의 장단점</h5><ul>
<li>장점: Bottleneck 구조는 갖고 있어서 핵심 Feature를 추출해낼 수 있으며 Domain이 다른 이미지로 변형을 하고 싶을 때 적합하다.</li>
<li>단점: 생성(복원)된 이미지의 Detail이 떨어진다.</li>
</ul>
<h5 id="Bottleneck-구조를-갖는-AutoEncoder를-사용한-이유"><a href="#Bottleneck-구조를-갖는-AutoEncoder를-사용한-이유" class="headerlink" title="Bottleneck 구조를 갖는 AutoEncoder를 사용한 이유"></a>Bottleneck 구조를 갖는 AutoEncoder를 사용한 이유</h5><p>추가적으로 Bottleneck 구조를 갖는 AutoEncoder를 사용한 이유는 Anomaly Detection tasks에서 defective structures를 image distribution에서 제외하는 것은 반복적으로 발생하는 문제라고 합니다. 그래서 Bottleneck 구조를 갖는 AE로 Feature map을 압축하는 과정은 Normal Image를 Manifold에 놓여지도록 reconstruction을 일반화 합니다.</p>
<h3 id="Unet"><a href="#Unet" class="headerlink" title="Unet"></a>Unet</h3><p>U-Net 구조의 특징은 “Skip-connection이 잇다보니 입력된 영상에 대한 detail들이 마지막 layer 까지 잘 전달 된다는 특징”이 있습니다. 그래서 아무래도 AutoEncoder의 결과와 비교해보면 output image quality가 더 좋습니다. 그렇지만 단점도 존재합니다. “Skip-connection은 depth가 거의 없다보니 depth가 어느정도 있는 다른 네트워크 구조에 비해 생성된 결과가 별로”라는 점 입니다. </p>
<h5 id="U-Net-구조의-특징과-장단점"><a href="#U-Net-구조의-특징과-장단점" class="headerlink" title="U-Net 구조의 특징과 장단점"></a>U-Net 구조의 특징과 장단점</h5><ul>
<li>특징: paired된 dataset이 어느정도 비슷한 컨텐츠들이 있는 경우 skip-connection을 많이 사용하는 경향을 보이고 있다.</li>
<li>장점: 처음 detail들이 마지막 layer까지 잘 전달 된다.</li>
<li>단점: skip-connection을 사용해서 depth가 거의 없다.</li>
</ul>
<h3 id="Bottleneck-구조를-갖는-AutoEncoder에-Skip-connection을-사용한-이유"><a href="#Bottleneck-구조를-갖는-AutoEncoder에-Skip-connection을-사용한-이유" class="headerlink" title="Bottleneck 구조를 갖는 AutoEncoder에 Skip-connection을 사용한 이유"></a>Bottleneck 구조를 갖는 AutoEncoder에 Skip-connection을 사용한 이유</h3><p>AE skip-connection architecture와 같이 Skip-connections을 사용하면 처음 detail들이 마지막 layer까지 전달되다 보니 reconstruction image가 보다 더 선명해 진다고 합니다. 근데 <code>U-Net에서 Skip-connection을 사용할때는 Decoder 구조에 feature map을 Concatenation</code>해서 사용했지만 해당 논문에선 <code>encoder에서 decoder로 feature map을 Addition</code> 시켜주었다고 합니다.</p>
<p>red box에서 다시 설명을 하겠지만, Skip-connection이 없는 그냥 AE를 사용해서 얻은 reconstruction image를 query image랑 MSE로 loss를 구하면 reconstruction image가 나중에는 결국 blurry한 image로 reconstruction됩니다. 또한 reconstruction loss가 커지게 되어서 엽력 이미지랑 모양이 똑같은 이미지로 복원이 잘 안이루어지게 됩니다.</p>
<h3 id="GAN-말고-AESc를-사용한-이유"><a href="#GAN-말고-AESc를-사용한-이유" class="headerlink" title="GAN 말고 AESc를 사용한 이유"></a>GAN 말고 AESc를 사용한 이유</h3><p>위에서도 언급한 내용으로 본 논문에서는 Clean(Normal) image를 복원 시키는 과정에서 GAN의 mode collapse 문제점을 지적하며 GAN 말고 Skip-connection을 사용한 AutoEncoder(AECs) 구조를 사용했습니다.</p>
<p>Anomaly Detection은 Unsupervised Learning으로 접근하고 있으며 그중에 대표적인 방법 중 GAN을 사용 안하는 이유는 딱 3가지 입니다.</p>
<ul>
<li>mode collapse로 인해 학습 하기가 어렵다.</li>
<li>generative distribution에서 결합 샘플을 제외 못함 -&gt; 이 문제는 AE에서 Bottleneck 구조로 해결.</li>
<li>inference 과정시 query image의 distribution과 latent space에 속하는 가장 유하산 출력 이미지를 생성해야 하는 latent space 속의 latent vector를 찾기 위해서 추가적인 최적화 단계가 필요.</li>
</ul>
<p>비교를 위해 AnoGAN과 여기서 제안한 방법을 비교해보았는데 당연히(?) 이 논문에서 제안한 방법이 더 잘 나왔다 라는 결과가 나와서 GAN 사용 안하고 AES + Stain Model을 사용한 것 같습니다. </p>
<p><strong><em>🤔 개인적으로 여기서 의문이 드는 점은 AnoGAN의 GAN Architecture는 DCGAN으로 이루어져 있습니다. DCGAN은 mode collapse를 방지할 수 없어서 개선된 모델들이 더 많이 나왔습니다. 근데 굳이 64x64 이미지로 학습을 진행한 AnoGAN과 비교를  하다니…. 의문점이 많다는 생각이 들었습니다.</em></strong></p>
<img src="/images/post_images/post0003/figure_0004.png" width="100%" height="100%">

<p>위 결과 그림은 AE와 AESc 모델 각각 Stain noise model를 추가한 것과 추가하기 전 reconstruction image 결과 비교한 그림입니다. AutoEncoder와 Skip-connections를 사용한 AutoEncoder 모델 각각 Identity mapping을 방지하기 위해 넣어준 Stain noise model과 아무것도 안넣어준 경우의 reconstruction image의 결과를 비교한 그림입니다. <code>None의 경우 Identity mapping이 이루어져서 입력 이미지 그대로 출력되어 제대로 결측인 부분들은 감지하지 못합니다. 그러나 Stain noise model을 추가한 결과를 보면 Identity mapping을 방지해서 결함이 있는 이미지가 들어오더라도 구조는 동일하게 가지만 정상인 이미지로 reconstruction</code>하는 것을 볼 수 있습니다. 다만 <code>skip connections을 사용하지 않아서 AE기반인 모델들은 이미지가 blurry</code>하게 나오는 것을 볼 수 있습니다.</p>
<p>AESc를 보면  AE와는 다르게 image가 선명하게 복원되는 것을 볼 수 있으며  Stain model을 사용한 결과가 Identity mapping이 안이루어진 모습을 확인할 수 있습니다.</p>
<hr>
<p>위에서 설명한 Train 과정에서 있는 중요 내용들을 요약하면 다음과 같습니다.</p>
<ul>
<li><p>GAN의 mode collapse 문제로 AutoEncoder 구조를 사용해서 Normal Image를 reconstruction 하도록 진행.</p>
</li>
<li><p>AE는 Bottleneck 구조를 갖는다. 따라서 Feature map을 압축하는 과정이 Normal Image Manifold에 놓여지도록 reconstruction을 일반화 시킬 수 있다.</p>
</li>
<li><p>AutoEncoder만 사용하면 Identity mapping이 발생되므로 Stain noise model을 사용했다.</p>
</li>
<li><p>AutoEncoder에 Skip-connections을 추가하므로 써 reconstruction image가 blurry하게 복원 되는 것을 방지했으며 U-Net과의 차이점은 concatenation 말고 Addition 시켜주었다는 점이다</p>
</li>
</ul>
<hr>
<hr>
<h1 id="4-Test-Step"><a href="#4-Test-Step" class="headerlink" title="4.Test Step"></a>4.Test Step</h1><h3 id="Red-box"><a href="#Red-box" class="headerlink" title="Red box"></a>Red box</h3><p>red box에 있는 내용은 test data를 이용해서 inference하는 과정입니다. 여기서 눈여겨 볼 점은 inference할 때 두가지의 전략(<code>Residual-based detection과 Uncertainity-based detection</code>)으로 진행 했습니다.</p>
<h2 id="4-1-Residual-based-Detection"><a href="#4-1-Residual-based-Detection" class="headerlink" title="4-1. Residual-based Detection"></a>4-1. Residual-based Detection</h2><p>Test과정 중 Query image랑 Query image를 기반으로 Clean(Normal) image로 reconstruction된 image와 차이를 계산하는 것 입니다. 이때 사용되는 loss는 L2Norm(MSE)이고, Image-wise 관점과 Pixel-wise 관점에서 이루어 집니다.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">• residual(잔차)</span><br><span class="line">→  모집단에서 추출한 표본둘의 평균(표본평균)과 개별 표본갑 간의 &#39;편차&#39;를 말하지만 주로 &#39;추정오차 (Estimation Error)&#39;와 거의 같음 의미를 지닌다.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">• 추정 오차 (Estimation Error)</span><br><span class="line">→  표본 집단에 기초해 산출된 기대값(추정값)과 확률 시행 결과의 관측값 과의 차이.</span><br></pre></td></tr></table></figure>

<h3 id="Residual-baded-detection의-단점과-MCDropout-사용-이유"><a href="#Residual-baded-detection의-단점과-MCDropout-사용-이유" class="headerlink" title="Residual-baded detection의 단점과 MCDropout 사용 이유"></a>Residual-baded detection의 단점과 MCDropout 사용 이유</h3><p>Anomaly Detection 하고싶은 부분 말고 <code>나머지 뒷 배경과의 대조가 명확하지 않은 경우</code>. Normal Image로 Reconstruction을 해도 reconstruction loss가 충분하게 높지 않게 됩니다. 따라서 <code>MCDropout을 사용해서 prediction uncertainity을 정량화 하여 anomaly detection에 사용</code>합니다.</p>
<h2 id="4-2-Uncertainty-based-Detection"><a href="#4-2-Uncertainty-based-Detection" class="headerlink" title="4-2. Uncertainty-based Detection"></a>4-2. Uncertainty-based Detection</h2><p>Uncertainity-based detection 방법은 Bayesian Estimation에서 나온 개념입니다. <code>Uncertainty(불확실성)는 확률 변수의 분산 크기이며 확률 변수가 얼마나 random한지 측정하는 sclar값</code> 입니다. 그래서 해당 값은 Bayesian Model을 이용한 Estimation에서 확인할 수 있는데 Bayesian Model의 parameter 수가 많아서 model이 많이 무겁다고 합니다. 그래서 이와 비슷한 효과를 주는 방법에서 <code>Uncertainity를 정량화 하는 방법으로 MCDropout(Monte Carlo Dropout)을 사용해서 Uncertainity를 정량화</code> 한다고 합니다.</p>
<p>그래서 <code>MCDropout으로 추정한 30개의 output image 사이의 variance(분산)으로 추정할 때 훈련중에 볼 수 없는 structures, 즉, 이상징후가 더 높은 불확실성(uncertainties)과 상관성을 갖는 직관에 의존하는 방법</code>입니다. 그래서 해당 논문에서는 AutoEncoder의 layer가 깊어질수록 dropout level 을 증가하면서 [0, 0, 10, 20, 30, 40] 적용한 결과 더 정확한 검출이 얻어지는 것을 밝혀냈다고 합니다.</p>
<img src="/images/post_images/post0003/figure_0005.png" width="100%" height="100%">

<p>위 그림은 reconstruction residual가 대부분 uncertainty와 상관관계가 있음을 보여주는 그림 입니다. 첫번째, 두번째 행은 Stain noise model을 사용하지 않고 AE and AECs networks를 학습한 경우이고 세번째, 네번째 행은 Stain noise model을 사용한 경우입니다. 두가지 경우 공통적으로 test 할 때 MCDropout을 사용 안한 상태로 Residual-based detection과 Uncertainty-based detection한 결과를 나타내고 있습니다. </p>
<h3 id="데이터셋-상황에-따른-Detection-방법-선택"><a href="#데이터셋-상황에-따른-Detection-방법-선택" class="headerlink" title="데이터셋 상황에 따른 Detection 방법 선택"></a>데이터셋 상황에 따른 Detection 방법 선택</h3><p>Residual-based Detection은 Reconstruction error threshold를 넘어가면 결함으로 간주하는 원리입니다. AECs가 Normal 한 이미지를 reconstruction을 하도록 학습하는 과정은 결함이 있는 구조를 깨끗한 이미지로 대처하도록 하는 것이 목적이지 그 주변 환경과의 대비를 더 명확하게 주라고 학습을 한 것은 아닙니다. 그래서 reconstruction image가 주변 환경과 대조가 잘 되지 않으면 residual intensities가 낮게 나옵니다.</p>
<p>반면에 Uncertainity-based Detection은 이미지의 구조와 주변 환경의 대비에 의존하지 않는다는게 가장 큰 특징입니다. 따라서 대비가 늦은 결함 이미지의 경우 Anomaly Detection 기능이 향상됩니다.</p>
<p>정리를 한번 해보도록 하겠습니다.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">주변환경과 데이터셋 대비가 명확하다</span><br><span class="line">→  Residual-based Detection 전략이 좋다.</span><br><span class="line"></span><br><span class="line">주변 환경과 데이터셋 대비가 명확하지 않다.</span><br><span class="line">→  Uncertainity-based Detection 전략이 좋다.</span><br></pre></td></tr></table></figure>
<p>추가적으로 위에서도 언급했다 싶이 Residual-based Detection 전략으로 진행할 경우 AESc + Stain model은 일반적으로 산발적인 반점으로 구성 된 reconstruction residual을 유발하고 낮은 대비를 갖는 데이터셋에 대해 결함을 놓치게 됩니다. 이런 경우에는 Uncertainity-based 전략이 효과적입니다.</p>
<hr>
<h1 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5.Conclusion"></a>5.Conclusion</h1><p>Query image에서 Normal image를 reconstruction 하는 것을 기반으로 한 anomaly detection 방법을 진행하기 위해서 본 논문에서는 <code>Skip-connection을 사용한 AutoEncoder인 AESc를 기반으로 MCDropout으로 30번 estimated된 reconstruction residual 또는 prediction uncertainity에 의존하여 Anomaly Detection을 진행</code>합니다.</p>
<p>Skip-connections를 Addition 시켜 사용한 AutoEncoder 구조를 사용할 때 장접을 본 논문에서는 입증을 하였으며 <code>Identity mapping이 이루어지지 않도록 train image가 Stain Noise model로 corrupted 시켜서 학습을 진행</code>하였습니다.</p>
<p>또한 본 논문에서 사용된 새로운 접근 방법은 일반 AutoEncoder보다 상당히 잘 MVTec AD Dataset들의 결함들을 잘 검출 해냈으며 AECs + Stain noise model을 사용하여 AutoEncoder와의 Uncertainity-based Detection 전략을 공정하게 비교해냈습니다.</p>
<p>Reconstruction residual과 달리 Uncertainity Indicator는 결함과 그 주변 사이 환경의 대비와는 무관합니다. 따라서 주변 환경과 대비가 뚜렷하다면 Reconstruction residual 전략으로 가고 대비가 뚜렷한 대비가 없다는 Uncertainity 기반 전략으로 Anomaly Detection을 진행하면 됩니다.</p>
<p>또한 Residual-based detection 전략에 비해 Uncertainty-based detection 전략은 Normal한 query image에서 false-positive rate를 증가키시게 되는 단점이 존재합니다.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="ko">
    <link itemprop="mainEntityOfPage" href="http://mulkong.github.io/2021/05/05/F-AnoGAN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="mulkong">
      <meta itemprop="description" content="공부한 내용을 바탕으로 정리한 기술블로그 입니다.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mulkong DeepLearning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/05/05/F-AnoGAN/" class="post-title-link" itemprop="url">[F-AnoGAN]Fast unsupervised anomaly detection with generative adversarial networks</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">작성일</span>
      

      <time title="Post created: 2021-05-05 15:09:09 / Updated at: 17:29:50" itemprop="dateCreated datePublished" datetime="2021-05-05T15:09:09+09:00">2021-05-05</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/GAN/" itemprop="url" rel="index"><span itemprop="name">GAN</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <hr>
<ul>
<li>GAN을 사용한 최초 Anomaly Detection 방법인 AnoGAN의 후속 모델로 Encoder 모델을 사용하여 더 빠르게 $G(x)$와 $x$를 latent space안에 mapping 시켜 Anomaly Detection 하는 방법입니다.</li>
<li>Paper 원문: f-AnoGAN: <a target="_blank" rel="noopener" href="https://www.sciencedirect.com/science/article/abs/pii/S1361841518302640">Fast unsupervised anomaly detection with generative adversarial networks</a></li>
<li>f-AnoGAN tutorial code(Pytorch): <a target="_blank" rel="noopener" href="https://github.com/mulkong/f-AnoGAN_with_Pytorch">Tutorial Link</a></li>
</ul>
<hr>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><h2 id="문제점"><a href="#문제점" class="headerlink" title="문제점"></a>문제점</h2><p>정확한 annotation은 시간이 많이 들기 때문에 전문가(방사선사, 의사 등)가 clinical imaging(임상 영상)을 직접 annotation을 표시한 데이터를 얻는 것은 어렵다. 또한 모든 병변에 대해 annotation이 표시되지 않을 수 있으며 annotation에 대해서 정확하게 이 병변이 어떤 병변인지 설명 되어있지 않은 경우도 있습니다.</p>
<h2 id="지도-학습-supervised-learning-의-장단점"><a href="#지도-학습-supervised-learning-의-장단점" class="headerlink" title="지도 학습(supervised learning)의 장단점"></a>지도 학습(supervised learning)의 장단점</h2><p>전문가로부터 분류된 training data를 받아 Supervised Learning 방식으로 모델을 학습 시키면 좋은 성능을 얻는 반면, annotation이 표시된 병변으로만 제한이 되는게 단점이다.</p>
<h2 id="비지도-학습-unsupervised-learning-으로-접근한-f-AnoGAn-제안"><a href="#비지도-학습-unsupervised-learning-으로-접근한-f-AnoGAn-제안" class="headerlink" title="비지도 학습(unsupervised learning)으로 접근한 f-AnoGAn 제안"></a>비지도 학습(unsupervised learning)으로 접근한 f-AnoGAn 제안</h2><p>본 논문에서는 biomarker candidates를 할 수 있는 anomalous images 및 image segments를 식별할 수 있는 GAN 기반 Unsupervised Learning 접근법으로 해결하는 f-AnoGAN(fast AnoGAN)을 제안한다.</p>
<h2 id="Fast-mapping-technique-of-new-data"><a href="#Fast-mapping-technique-of-new-data" class="headerlink" title="Fast mapping technique of new data"></a>Fast mapping technique of new data</h2><p>Normal data로 Generator model을 학습 시키고 GAN latent space에서 query data $X$(병변 유무를 확인하고 싶은 테스트 영상)의 fast mapping technique를 제안하고 평가한다. latent space mapping 방법은 Encoder를 기반으로 이루어지며 Discriminator feature residual error 및 image reconstruction error $G(z)$를 포함하는 훈련된 모델을 기반으로 anomaly detection이 진행된다.</p>
<p>Optical Coherence Tomography(OCT) 촬영 데이터를 사용한 딥러닝 학습 관련 실험은 본 논문에서 제안한 방법과 대체 접근법(AnoGAN, BiGAN 등)과 비교하며 실험한다. 본 논문에서 제안한 f-AnoGAN 방법이 anomaly detection의 정확도를 높인다는 포괄적인 경험적 증거를 제공한다. 또한 두 명의 망막 전문가를 대상으로 한 시각적 테스트 결과 생성된 이미지 $G(z)$가 실제 OCT 이미지와 구별이 잘 안되는 것으로 나타났다.</p>
<p><img src="/images/post_images/post0002/f-AnoGAN_figure1.png" alt="그림1"></p>
<center><figcaption> (그림 1) </figcaption></center>


<h1 id="Fast-GAN-based-anomaly-detection"><a href="#Fast-GAN-based-anomaly-detection" class="headerlink" title="Fast GAN-based anomaly detection"></a>Fast GAN-based anomaly detection</h1><p>본 논문에서 제안한 Anomaly Detection 순서는 크게 두가지로 이루어 집니다.</p>
<ul>
<li>normal image만 이용해서 GAN 학습</li>
<li>normal image을 잘 생성할 수 있는 GAN을 기반으로 GAN 학습할 때 사용한 데이터와 동일한 데이터로 Encoder 학습 </li>
</ul>
<p>f-AnoGAN의 핵심은 <code>image를 latent space mapping하는 방법으로 Encoder 모델을 사용한 것</code>이 핵심입니다. 비슷한 방법인 AnoGAN과 비교를 해보면 확실한 차이점을 확인할 수 있습니다.</p>
<h2 id="AnoGAN"><a href="#AnoGAN" class="headerlink" title="AnoGAN"></a>AnoGAN</h2><p><a href="https://mulkong.github.io/2020/10/04/Unsupervised-Anomaly-Detection-With-GAN/">🔗 AnoGAN 정리</a></p>
<p>AnoGAN의 목표는 query image XX가 주어지면 query image $X$와 가장 유사하고 Manifold $X$에 위치하는 $G(z)$의 latent space를 찾는 것 입니다. 이 과정에서 $G(z)$의 Probability density function에 의존하게 됩니다.</p>
<p>mapping된 latent space의 세부적인 과정은 다음과 같습니다.</p>
<ul>
<li>random하게 $z_n$을 뽑아 $G(z_n)$을 얻는다.</li>
<li>query image $X$와 유사한 $G(z)$을 찾기 위해 latent space안에서 최적의 $z$을 찾기 위해 residual loss + discriminator loss을 기반으로 backpropagation을 통해 찾는다. (OCT 기분 500 iteration)</li>
<li>최종적으로 residual loss 수식을 그대로 사용하여 anomaly score을 계산한다.<blockquote>
<p>anomaly score 구하는 방법은 f-AnoGAN에서도 동일하게 적용 됩니다.</p>
</blockquote>
</li>
</ul>
<p>위에서 설명드린 방법은 학습 이미지 크기가 작으면 효율적이고 나름 빠르게 진행이 되지만 이미지 크기가 크면 클수록 그만큼 많은 정보들을 고려해야 하므로 random하게 iteration하며 학습하는 AnoGAN 방법은 mapping이 제대로 안되는 경우가 발생할 수 있습니다.</p>
<h2 id="F-AnoGAN"><a href="#F-AnoGAN" class="headerlink" title="F-AnoGAN"></a>F-AnoGAN</h2><p>f-AnoGAN은 이런 문제를 해결하고자 AutoEncoder에서 아이디어를 얻었다고 할 수 있습니다. AutoEncoder는 입력된 정보들을 잘 설명할 수 있는 latent vector로 차원을 압축 시켰다가 Decoder를 통해 복원하는 간단한 모델입니다. 이 과정에서 사용된 Encoder 모델을 f-AnoGAN에서는 image를 latent space에 mapping하는 모델로 사용된 것 입니다. 이렇게 학습 데이터를 <code>latent space에 mapping 시켜주는 과정은 identity transformation(항등변환)</code> 해주는 과정이라고 할 수 있으며 입력 이미지가 들어가면 그대로 Decoder를 통해 동일한 이미지로 복원하는 identity mapping이라고도 할 수 있습니다.</p>
<h2 id="Latent-space-mapping"><a href="#Latent-space-mapping" class="headerlink" title="Latent space mapping"></a>Latent space mapping</h2><p>GAN 학습은 latent space $Z$ $\rightarrow$ Manifold $X$로 mapping되는 $G(z) = z \rightarrow x$을 생성하지만 Anomaly Detection에서 필요한 $X \rightarrow$ $Z$로 inverse mapping은 불가능 합니다. (이 개념은 AnoGAN 논문에서도 언급된 개념입니다.)</p>
<p>inverse mapping이 안되는 문제점을 해결하고자 AnoGAN에서는 위에서 언급한 방법으로 latent space에 mapping을 해주었지만 f-AnoGAN에서는 Deep Encoder Network를 사용하여 $E(x) = x \rightarrow z$ 즉, inverse mapping이 가능하도록 학습을 진행합니다. 이 방법을 사용하여 f-AnoGAN제안한 방법은 크게 2가지 방법입니다.</p>
<ul>
<li>$z \rightarrow image \rightarrow z$</li>
<li>$image \rightarrow z \rightarrow image$</li>
</ul>
<p>이 두가지 방법 모두 (그림 2)처럼 image에서 latent space ZZ로 mapping 하는 방법입니다.</p>
<p><img src="/images/post_images/post0002/f-AnoGAN_figure2.png"></p>
<center><figcaption> (그림 2) </figcaption></center>

<h3 id="⎮-Training-the-encoder-with-generated-images-ziz-architecture"><a href="#⎮-Training-the-encoder-with-generated-images-ziz-architecture" class="headerlink" title="⎮ Training the encoder with generated images: $ziz$ architecture"></a>⎮ Training the encoder with generated images: $ziz$ architecture</h3><p>학습중 latent space $Z$로 부터 random sampling된 latent vector $z$는 더이상 weight가 update 안되는 fixed weight $G$를 통해 image space에 mapping 되고 Encoder는 이를 다시 latent space $Z$에 mapping 하도록 학습 합니다. 따라서 $ziz$ architecture의 경우 GAN 학습할 때 사용했던 normal image가 필요 없습니다.</p>
<p>$ziz$ architecture로 학습할 때 사용되는 loss는 $z$와 $E(G(z))$의 MSE를 최소화 하는 방법으로 학습이 진행됩니다.</p>
<p>$$ L_{ziz}(z) = \cfrac{1}{d}||z - E(G(z))||^{2} $$</p>
<p>$ziz$ architecture 방법은 학습할 때 생성된 이미지만 잘 mapping될 뿐 query image $X$가 들어오면 제대로 mapping을 못하게 된다는 단점이 있어 f-AnoGAN 논문에서는 해당 방법으로 학습을 진행 안합니다..</p>
<h3 id="⎮-Training-the-encoder-with-real-images-izi-architecture"><a href="#⎮-Training-the-encoder-with-real-images-izi-architecture" class="headerlink" title="⎮ Training the encoder with real images: $izi$ architecture"></a>⎮ Training the encoder with real images: $izi$ architecture</h3><p>학습하는 동안 real image에서 latent space $Z$로 encoding되어 mapping하는 과정은 Encoder model로 이루어 집니다. 이런 과정을 통해서 query image $X$가 들어오면 AnoGAN에서 사용한 iteration을 시켜주는 방법 없이 빠르게 $X$와 구조적으로 일치한 $G(E(x))$를 생성할 수 있게 됩니다.</p>
<p>$izi$ architecture로 학습할 때 사용되는 Losss는 $x$와 $E(G(x))$의 MSE를 최소화 하는 방법으로 학습이 진행됩니다.</p>
<p>$$ L_{izi}(x) = \cfrac{1}{n}||x - G(E(x))||^{2} $$</p>
<p>$izi$ architecture는 GAN을 학습할 때 사용한 학습 데이터를 이용해서 학습이 진행 됩니다.</p>
<h4 id="izi-architecture의-단점"><a href="#izi-architecture의-단점" class="headerlink" title="$izi$ architecture의 단점"></a>$izi$ architecture의 단점</h4><p>$X$가 latent space $Z$에서 정확하게 어디에 위치하는지 $ziz$ architecture는 $G(z)$를 바로 latent space에 mapping 시켜서 알 수 있었지만 $izi$ architecture는 알 수 없습니다. 따라서 image space로 다시 mapping 하고 image−to−image residual loss를 계산하여 image−to−$z$ mapping의 정확도를 간접적으로 측정할 수 밖에 없습니다.</p>
<p>이런 방법으로 진행하면 query image로 normal image가 들어오면 괜찮지만 abnormal image가 들어오면 residual 이 적은 이미지가 생성될 수 있습니다. 따라서 이런 문제점으로 본 논문에서는 feature space를 loss로 사용해서 mapping이 이루어질 수 있도록 $izif$ architecture를 고안하게 되었습니다.</p>
<h3 id="⎮-Discriminator-guided-izi-encoder-training-izif-architecture"><a href="#⎮-Discriminator-guided-izi-encoder-training-izif-architecture" class="headerlink" title="⎮ Discriminator guided $izi$ encoder training: $izif$ architecture"></a>⎮ Discriminator guided $izi$ encoder training: $izif$ architecture</h3><p>$izi$ architecture 문제점을 해결하기 위해 Discriminator model의 중간 계층인 feature space에서 residual loss를 추가적으로 사용하여 Encoder model을 학습합니다</p>
<p>$$ L_{izif}(x) = \cfrac{1}{n}\cdot||x - G(E(X))||^{2} + \cfrac{k}{n_d}\cdot||f(x) - f(G(E(x)))||^{2}  $$</p>
<blockquote>
<p>nd: 중간 계층의 feature space의 dimension<br>k: weight factor</p>
</blockquote>
<p>수식을 보면 Discriminator feature space는 AnoGAN에서 사용된 loss와 관련이 있는것을 볼 수 있습니다. Discriminator feature는 GAN을 학습할 때 얻은 parameter들을 사용한 것 입니다. feature space를 loss로 사용하게 되면 image space와 latent space에서 잘 mapping이 되도록 약간의 guide를 제공해주는 역할을 하게 됩니다.</p>
<h2 id="Detection-of-anomalies"><a href="#Detection-of-anomalies" class="headerlink" title="Detection of anomalies"></a>Detection of anomalies</h2><p>GAN도 학습이 잘 되었고 Encoder도 학습이 잘 되었다면 이제 query image $X$를 입력으로 넣어서 anomaly score를 계산하면 됩니다. anomaly score를 계산하는 것은 image level의 anomaly detection중 query image와 reconstruction image의 deviation을 score로 나타내야 합니다.</p>
<p>이 score를 anomaly score라고 하며 수식은 아래와 같습니다.</p>
<p><img src="/images/post_images/post0002/f-AnoGAN_figure3.png" alt="그림1"></p>
<center><figcaption> (그림 3) </figcaption></center>

<p>수식을 보면 $loss_{izif}$ 수식과 동일한 것을 볼 수 있습니다.</p>
<p>일반적으로 수식 모두 abnormal image에서 높은 anomaly score, normal image는 낮은 anomaly score가 계산됩니다. 모델은 normal image에 대해서만 학습하므로 입력 이미지와 시각적으로 유사한 이미지만 normal image의 Manifold $X$에 눞혀서 Encoder를 통해 재구성 됩니다.</p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="ko">
    <link itemprop="mainEntityOfPage" href="http://mulkong.github.io/2020/10/04/Unsupervised-Anomaly-Detection-With-GAN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="mulkong">
      <meta itemprop="description" content="공부한 내용을 바탕으로 정리한 기술블로그 입니다.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Mulkong DeepLearning">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2020/10/04/Unsupervised-Anomaly-Detection-With-GAN/" class="post-title-link" itemprop="url">[AnoGAN]Unsupervised Anomaly Detection with GAN</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">작성일</span>

      <time title="Post created: 2020-10-04 19:01:01" itemprop="dateCreated datePublished" datetime="2020-10-04T19:01:01+09:00">2020-10-04</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Updated at: 2021-05-05 14:59:10" itemprop="dateModified" datetime="2021-05-05T14:59:10+09:00">2021-05-05</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/GAN/" itemprop="url" rel="index"><span itemprop="name">GAN</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <hr>
<ul>
<li>이번 포스팅은 티스토리에서 깃블로그로 이사 후 티스토리에 정리했던 내용에서 내용을 추가해서 작성한 글 입니다. 이전 글은 <a target="_blank" rel="noopener" href="https://sensibilityit.tistory.com/506?category=731657">AnoGAN 정리글_Tistory</a>에서 보실 수 있습니다.</li>
<li>기존 Anomaly Detection은 Supervised Learning으로 접근했지만 AnoGAN은 GAN을 이용한 Unsupervised Learning 방법으로 접근하여 Anomaly Detection하는 논문입니다.</li>
<li>Paper 원문: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1703.05921">Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery</a></li>
</ul>
<hr>
<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><h2 id="GAN이란"><a href="#GAN이란" class="headerlink" title="GAN이란?"></a>GAN이란?</h2><p>GAN 학습 과정은 Discriminator가 Real/Fake를 잘 맞추도록 학습한 후 Generator가 생성한 Fake image가 Discriminator를 속여 Real이라고 말하게끔 하도록 하는 학습 과정입니다.</p>
<h2 id="DCGAN"><a href="#DCGAN" class="headerlink" title="DCGAN"></a>DCGAN</h2><h3 id="⎮-CNN-vs-MLP"><a href="#⎮-CNN-vs-MLP" class="headerlink" title="⎮ CNN vs MLP"></a>⎮ CNN vs MLP</h3><ul>
<li><p>MLP(Multi-Layer Perceptron)</p>
<ul>
<li>특징: 3차원 데이터를 1개의 Vector로 풀어서 인식합니다. </li>
<li>단점: 이미지 위치 정보를 무시하게 됩니다. (Feature loss 발생)</li>
</ul>
</li>
<li><p>CNN(Convolutional Nerual Network)</p>
<ul>
<li>특징: 3차원 데이터 입력을 그대로 사용해 위치 정보가 반영됩니다.</li>
</ul>
</li>
</ul>
<h3 id="⎮-Discriminator"><a href="#⎮-Discriminator" class="headerlink" title="⎮ Discriminator"></a>⎮ Discriminator</h3><p>Convolution 연산을 통해 데이터의 차원을 줄이는 과정입니다. 어떻게 보면 Encoder의 모델 구조와 비슷하다고 볼 수 있습니다.</p>
<h3 id="⎮-Generator"><a href="#⎮-Generator" class="headerlink" title="⎮ Generator"></a>⎮ Generator</h3><p>Generator는 Latent Space를 입력으로 넣어주면 차원을 확장 시키는 방법으로 모델 구조를 구성합니다. 이때 사용하는 함수는 Pytorch 기준 nn.ConvTranspose2d를 사용합니다. 즉, Deconvolution 방식으로 모델이 구성됩니다.</p>
<h3 id="⎮-DCGAN-특징"><a href="#⎮-DCGAN-특징" class="headerlink" title="⎮ DCGAN 특징"></a>⎮ DCGAN 특징</h3><ol>
<li><p><strong>Generator는 이미지를 외워서 보여주는 것이 아니다. (Memorization이 안일어난다.)</strong><br> Paper를 읽어보면 “walking in the latent space”라는 문구가 나옵니다. 만약 G가 Memorization이 발생된다면 latent vector를 조금씩 바꿀 때 이미지가 변형되 못할 것 입니다.</p>
</li>
<li><p><strong>Memorization이 일어난 경우는 어떤 경우인가?</strong><br> Generator가 유의미한 특징들을 학습하지 않고 overfitting이 발생되 데이터와 1:1 mapping이 발생하는 Identity Mapping 학습이 이루어지게 됩니다.</p>
</li>
<li><p><strong>walking in the latent space</strong><br> latept space 내부에서 vector값을 조금씩 변경하면 부드럽게 이미지들이 변화되는것을 확인할 수 있는 그림 입니다.</p>
</li>
</ol>
<p><img src="/images/post_images/walking_latent_space_DCGAN.jpg"></p>
<h1 id="AnoGAN"><a href="#AnoGAN" class="headerlink" title="AnoGAN"></a>AnoGAN</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>의료데이터에서 disease progression과 treatment monitoring에 이미지를 검출하는 model을 구하는 것은 어려운 일이다. 기존 모델은 automating detection을 위해 annotation이 이루어진 많은 양의 데이터를 기반으로 Supervised Learning 방식으로 진행했다. Supervised Learning은 annotation과 이미 알려진 것들에 대해서만 detection이 이루어 지는 단점이 존재하다. 그래서 이 논문은 Unsupervised Learning 방식으로 학습을 진행한 후 Anomaly Detection 과정이 이루어진다. 이 논문에서 제안한 <strong>AnoGAN</strong>은 다양한 Normal Data로 학습한 DCGAN과 Image space 안에서 latent space의 mapping을 기반으로 Anomaly score를 계산한다. 새로운 데이터가 들어오게 되면 model은 anomaly한 부분에 labeling하게 되고 Normal image로 학습된 distribution에 적합한지 image patches에 anomaly score를 나타낸다. 이 방식은 망막의 광학 단층 촬영 영상에 적용한 결과 망막 유체 또는 반사성 초점을 포함하는 이미지와 같은 Abnormal image을 정확하게 식별함을 확인했다.</p>
<h2 id="AnoGAN이-등장한-계기"><a href="#AnoGAN이-등장한-계기" class="headerlink" title="AnoGAN이 등장한 계기"></a>AnoGAN이 등장한 계기</h2><p>이 논문에서 사용된 데이터는 의료 데이터 입니다. 의료현장, 산업현장 모두 공통적으로 Normal 데이터는 많지만 Abnormal 데이터는 부족한 경우가 많습니다. 지금까지 Anomaly Detection을 하기 위해 Supervised Learning 방법으로 진행했지만 이는 Anotation된 데이터가 많이 필요하다는 단점이 발생되게 됩니다. 그래서 Unsupervised Learning 방법 중 GAN을 이용한 Anomaly Detection을 진행하게 되었습니다.</p>
<p>좀 더 쉽게 정리를 해보면…<br>Discriminator는 입력 이미지가 True/False의 확률을 구하는 classifier라고 생각하시면 됩니다. 여기서 Unsupervised Learning으로 접근하는건 굳이 Annotation 과정을 할 필요 없이 모델 알아서 처리를 하는 과정으로 접근한다는 것 입니다. Discriminator을 학습할 때 그 discribution 안에 속하지 않은 데이터들은 다 Fake라고 하지 안을까? 라는 아이디어에서 Normal Discribution의 outlier detection을 할 수 있는 AnoGAN이 탄생하게 된 것 입니다.</p>
<h2 id="Unsupervised-Learning으로-접근한-이유"><a href="#Unsupervised-Learning으로-접근한-이유" class="headerlink" title="Unsupervised Learning으로 접근한 이유"></a>Unsupervised Learning으로 접근한 이유</h2><h3 id="⎮-Data-Imbalance-Problem"><a href="#⎮-Data-Imbalance-Problem" class="headerlink" title="⎮ Data Imbalance Problem"></a>⎮ Data Imbalance Problem</h3><p>위에서도 잠깐 언급을 하긴 했지만 의료데이터, 산업현장 데이터는 Normal 데이터는 많지만 Abnormal 데이터는 부족한 점이 특징입니다. 그래서 Supervised Learning으로 문제를 해결하려면 우선적으로 Data Imbalance 문제를 해결해야합니다. 이런 수고스러움을 덜고자 Unsupervised Learning 방법으로 접근하게 되었습니다.</p>
<h3 id="⎮-Annotation-작업의-엄청난-비용"><a href="#⎮-Annotation-작업의-엄청난-비용" class="headerlink" title="⎮ Annotation 작업의 엄청난 비용"></a>⎮ Annotation 작업의 엄청난 비용</h3><p>아무래도 의료데이터는 비전문가들이 annotation을 할 수 없다는 제한점이 있다보니 전문가들이 직접 annotation을 해야합니다. 해보신 분들은 하시겠지만 엄청난 시간과 비용을 투자해야한다는 단점이 발생합니다. 또한 균일하게 완벽하게 100%로 annotation을 한다는 보장이 없다보니 데이터 마다 성능차이가 많이 발생할 수 있다는 문제가 존재합니다. 그래서 이런 과정을 하지 않고 Normal 데이터로만 학습 한 후 Normal Discribution에서 벗어나는 데이터가 들어올때 어느 부분이 outlier인지 detection 해주는 방법으로 진행했습니다.</p>
<h2 id="AnoGAN의-동작-원리"><a href="#AnoGAN의-동작-원리" class="headerlink" title="AnoGAN의 동작 원리"></a>AnoGAN의 동작 원리</h2><h3 id="⎮-정상-데이터로-GAN-학습"><a href="#⎮-정상-데이터로-GAN-학습" class="headerlink" title="⎮ 정상 데이터로 GAN 학습"></a>⎮ 정상 데이터로 GAN 학습</h3><img src="/images/post_images/AnoGAN_step1.png" width="70%" height="70%">
Generator Model이 어떤 latent space가 들어와도 Normal 이미지를 잘 생성할 수 있도록 GAN을 Normal 데이터로만 학습시켜야 합니다. 그러면 Generator Model은 Normal Manifold를 학습하게 되어서 Normal 데이터만 생성하게 됩니다.

<h3 id="⎮-최적의-Z값-찾기"><a href="#⎮-최적의-Z값-찾기" class="headerlink" title="⎮ 최적의 Z값 찾기"></a>⎮ 최적의 Z값 찾기</h3><img src="/images/post_images/AnoGAN_step2.png" width="70%" height="70%">
Generator와 Discriminator의 Parameter를 Fix시켜 더이상 Update 해주지 않는 상태로 latent vector $z_1$을 random sampling 시킨 후 Generator에 입력 데이터로 넣어준다 $G(z_1)$. Generator는 Normal 데이터의 Manifold를 학습한 상태여서 Normal image를 생성하게 된다.


<h3 id="⎮-Residual-Loss"><a href="#⎮-Residual-Loss" class="headerlink" title="⎮ Residual Loss"></a>⎮ Residual Loss</h3><img src="/images/post_images/AnoGAN_step3.png" width="70%" height="70%">
query image와 $G(z_1)$ 사이에서 다른 부분이 있는지 그 차이를 비교하는 과정입니다.

<h3 id="⎮-Discrimination-Loss"><a href="#⎮-Discrimination-Loss" class="headerlink" title="⎮ Discrimination Loss"></a>⎮ Discrimination Loss</h3><img src="/images/post_images/AnoGAN_step4.png" width="70%" height="70%">
Discriminator의 역할은 True/False를 판별해주는 역할을 갖고 있습니다. 이런 기능을 이용해서 들어오는 입력 데이터들의 확률분포(Probability Distribution)을 파악해서 True/False를 판단해줄 수 있습니다. 즉, Discrimination Loss는 $G(z_r)$의 Manifold 또는 Data Distribution에 잘 Mapping되도록 패널티를 부과하는 Loss 입니다. 다만, 이때 Discriminator의 중간 레이어에서 뽑은 Feture를 이용해서 계산을 하고 있습니다. 그 이유는 논문에서 중간층이 더 많은 표현력을 갖고 있다고 나와있습니다. 이러한 과정을 Feature Mapping이라고 불립니다.


<p>$$<br>L_D(z_r) = \sum |f(x) - f(G(z_r))|<br>$$</p>
<h3 id="⎮-Residual-Discrimination-Loss"><a href="#⎮-Residual-Discrimination-Loss" class="headerlink" title="⎮ Residual + Discrimination Loss"></a>⎮ Residual + Discrimination Loss</h3><img src="/images/post_images/AnoGAN_step5.png" width="70%" height="70%">


<p>$$<br>L(z_r) = (1 - \lambda) \cdot L_R(z_r) + \lambda \cdot L_D(z_r)<br>$$</p>
<p>$z_1$에 대한 L(z_r)를 구하는 과정입니다. 이 과정은 Generatir와 Discriminator의 weight를 Fixed 시켜주고 $L(z_r)$가 최소가 되도록 latent vector를 Gradient Descent 과정을 통해서 조정 시켜줍니다. </p>
<p>따라서 $z_1 \rightarrow z_2 \rightarrow z_3 \rightarrow …. z_r$로 여러번 iteration 시켜주면서 제대로 query image와 $G(z)$와 mapping이 제대로 이루어지는 값을 찾는 과정을 진행합니다. (OCT 데이터셋은 500번 iteration 해주었습니다)</p>
<p>또한 $L(z_r)$를 Anomaly Score로 사용해서 판단의 기준으로 사용하게 됩니다.</p>
<h1 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h1><h2 id="DCGAN-1"><a href="#DCGAN-1" class="headerlink" title="DCGAN"></a>DCGAN</h2><p>DCGAN 원 논문에서 사용한 데이터는 RGB(3channels)이였습니다. 그렇지만 OCT 영상 데이터는 gray-scale(1channel)이여서 parameter만 변경을 시켜준 상태로 사용을 했습니다.<br><img src="/images/post_images/DCGAN.png" width="70%" height="70%"></p>
<h2 id="AnoGAN-1"><a href="#AnoGAN-1" class="headerlink" title="AnoGAN"></a>AnoGAN</h2><img src="/images/post_images/AnoGAN.png" width="70%" height="70%">

<h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>AnoGAN 전 과정을 요약 해보자면</p>
<ol>
<li>Normal Data로만 GAN을 학습 시킵니다. 잘 학습된 GAN은 Normal 이미지만 생성하게 됩니다.</li>
<li>latent space에서 $z_r$값을 random sampling 해줍니다.</li>
<li>Sampling해준 $z_r$을 이용해 query data와 비슷하게 생성되는지 $G(z_r)$와 query image와 비교를 합니다. 이 과정은 inference 과정에서 이루어짐니다.</li>
<li>query image와 $G(z_r)$이 비슷한 Distribution으로 mapping이 되었다면 비정상 query image가 들어왓다고 해도 $G(z_r)$은 정상이지만 query image와 구조적으론 똑같은 이미지로 생성이 될 것 입니다. 이 때 Anomaly Score를 계산해줍니다.</li>
</ol>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>





  



      

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

    </div>
  </main>

  <footer class="footer">
    <div class="footer-inner">
      

      

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">mulkong</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.0/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  


















  








  

  
      <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = '//cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
